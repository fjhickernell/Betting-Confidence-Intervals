\documentclass{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{paralist}

\newcommand{\art}[1]{\begingroup\color{blue}#1\endgroup}
\newcommand{\aadit}[1]{\begingroup\color{orange}#1\endgroup}
\newcommand{\fred}[1]{\begingroup\color{red}#1\endgroup}
\newcommand{\alexei}[1]{\begingroup\color{green}#1\endgroup}

% Lets make it look like writing on a blackboards
% not a 1970s typewriter!
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\emptyset}{\varnothing}

\input{FJHDef}

\newcommand{\real}{\mathbb{R}}


\newcommand{\e}{\mathbb{E}}
\newcommand{\bsa}{\boldsymbol{a}}
\newcommand{\bsx}{\boldsymbol{x}}
\newcommand{\bsone}{\boldsymbol{1}}
\newcommand{\bszero}{\boldsymbol{0}}

\newcommand{\simiid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\toas}{\stackrel{\mathrm{a.s.}}{\to}}

\newcommand{\runif}{\mathbb{U}}

\newcommand{\giv}{\!\mid\!} % 


\newcommand{\prplh}{\text{PrPl-H}}
\newcommand{\prpleb}{\text{PrPl-EB}}
\newcommand{\prplpm}{\text{PrPl}\pm}
\newcommand{\eb}{\mathrm{E}}

% commands in lower case give less carpal trouble
\newcommand{\mc}{\mathrm{MC}}
\newcommand{\rqmc}{\mathrm{RQMC}}
\newcommand{\hk}{\mathrm{HK}}

\newcommand{\rd}{\,\mathrm{d}}

\title{Empirical Bernstein and betting confidence intervals for randomized quasi-Monte Carlo}
\date{March 2025}
\author{List of authors}
\begin{document}
\maketitle
\begin{abstract}
This is a start on a draft article to pull together our various writings.    
\end{abstract}

\section{Introduction}

We study the combination of randomized quasi-Monte
Carlo (RQMC) integration 
with some non-asymptotic confidence interval methods.
RQMC has significant accuracy  benefits over plain Monte Carlo (that
we outline below), but so far most of the confidence intervals based on
RQMC estimates have had justifications that were asymptotic
in their sample size.  Empirical Bernstein and some related betting
methods (described below) give finite sample assurances for bounded integrands.  Specifically, if it is known that the integrand values must lie
between $0$ and $1$, then those methods can construct a confidence
interval that has at least $100(1-\alpha)\%$ probability to contain
the integral's value.  This confidence level holds for any integrand
satisfying the given bounds.  Any other finite interval known
to contain the integrand can, after linear transformation
be assumed to be the interval $[a,b]$.

The context for our work is as follows.
Many scientific problems require the numerical 
evaluation of multidimensional
integrals. When the dimension is high enough then classical rules 
like those in \cite{DavRab84} become too expensive to use. It is then
common to use plain Monte Carlo (MC) methods instead.  MC
methods based on random sampling typically have root mean
squared errors (RMSEs) of $O(n^{-1/2})$ given $n$
function evaluations.  This rate is slow but it is the
same in any dimension.  MC also allows one to use statistical
methods to quantify the uncertainty in the estimated
integral.  Quasi-Monte Carlo (QMC) 
methods (e.g.,  \cite{DicPil10a} and \cite{Nie92}) are deterministic
sampling strategies that under a bounded variation assumption
produce integral estimates with error $\tilde O(n^{-1})$ where
$\tilde O(\cdot)$ means we neglect powers of $\log(n)$.
Plain QMC methods are deterministic and so they lose the
uncertainty quantification advantage of MC. RQMC methods, 
surveyed in\cite{LEcLem02a}, allow one
one to make independent replicates of a statistically
unbiased QMC method to support variance estimates and
asymptotic confidence intervals.  RQMC methods can also
improve on the convergence rate of QMC methods obtaining
an RMSE of $\tilde O(n^{-3/2})$ under a smoothness assumption
\cite{Owe97,Owe08a} on the integrand.

Most confidence interval methods for RQMC are asymptotic
with coverage levels that converge to their desired level,
such as 95\%, as the amount of sampling increases.  The GAIL
project \cite{Gail_ug} provides an exception, so long as one can
provide some outside information on the integrand.  This could
be an assumption about a kurtosis quantity or it could be
an assumption about the way  in which Fourier-Walsh coefficients
decay.  Our goal here is to study RQMC confidence intervals
under the simple assumption that the integrand is bounded between
zero and one.  The advantage of this assumption over others is 
that in some applications we have no doubt as to whether the
assumption holds. We can then get finite sample confidence
intervals with coverage at least 95\% or some other desired level.

\section{Background}

TODO: things to define here: % add them here while drafting, sort 'em out later
\begin{compactitem}
\item $1\{A\}$
\item $\bszero$
\item confidence intervals and asymptotic confidence intervals 
\item confidence sequences
\item strict and asymptotically strict intervals
\item something about martingales (maybe some place else)
\end{compactitem}
\bigskip

The problem we consider is to approximate the finite-dimensional
integral
$$\mu = \int_{[0,1]^d}f(\bsx)\rd\bsx.$$
In MC sampling we take $\bsx_i\simiid \runif[0,1]^d$ and
estimate $\mu$ by
$$
\hat\mu = \frac1n\sum_{i=1}^nf(\bsx_i).
$$
Let $\sigma^2=\var(f(\bsx))$ for $\bsx\sim\runif[0,1]^d$.
We assume that $0<\sigma^2<\infty$. In that case the RMSE
of $\hat\mu$ is $\sigma/\sqrt{n}$. For $n\ge2$,
$$s^2 =\frac1{n-1}\sum_{i=1}^n(f(\bsx_i)-\hat\mu)^2$$
is an unbiased estimate of $\sigma^2$. Furthermore
$$
\lim_{n\to\infty}\Pr\Bigl( \sqrt{n}\frac{\hat\mu-\mu}s\le t^{1-\alpha}_{(n-1)}\Bigr) = 1-\alpha
$$
where $t^{1-\alpha}_{(n-1)}$ is the $1-\alpha$ quantile of
Student's $t$ distribution on $n-1$ degrees of freedom.
Then $\hat\mu \pm st^{1-\alpha/2}/\sqrt{n}$ is an
asymptotic $100(1-\alpha)$\% confidence interval for $\mu$.

\subsection{QMC and RQMC}

QMC sampling replaces random points $\bsx_i$ by 
deterministic points that more evenly sample the
unit cube.  There are many ways to quantify that 
property but the most widely used one is the
star discrepancy
$$
D_n^* = D_n^*(\bsx_1,\dots,\bsx_n)=\sup_{\bsa\in[0,1]^d}
\Bigl| \frac1n\sum_{i=1}^n1\{\bsx_i\in[\bszero,\bsa)\}
-\prod_{j=1}^da_j\Bigr|.
$$
Small values of $D_n^*$ show that all anchored boxes $[\bszero,\bsa)$
have very nearly the desired proportion of the $n$ points,
which is the volume of $[\bszero,\bsa)$.
It is possible to choose points $\bsx_1,\dots,\bsx_n$ so
that $D_n^*=\tilde O(n^{-1})$ while plain MC points
have $D_n^*=\tilde O(n^{-1/2})$ \cite{Nie92}.

The improvement in star discrepancy yields an improvement
in integration error via the Koksma-Hlawka inequality \cite{Hic04a}:
\begin{align}\label{eq:koksmahlawka}
|\hat\mu-\mu|\le D_n^*(\bsx_1,\dots,\bsx_n)\times V_{\hk}(f).
\end{align}
As described above $D_n^*$ is a measure of how non-uniform
the points $\bsx_i$ are. The new factor $V_{\hk}(f)$ is the
total variation of $f$ in the sense of Hardy and Krause.
See \cite{Owe05a}. It follows from~\eqref{eq:koksmahlawka} 
that we can get $|\hat\mu-\mu|=\tilde O(n^{-1})$
using QMC.

If we knew $D_n^*$ and $V_\hk(f)$, then~\eqref{eq:koksmahlawka} would provide a
perfect quantification of our uncertainty about $\mu$.
It would be non-asymptotic and even non-probabilistic.
Unfortunately, $D_n^*$ is generally very expensive
to obtain and $V_{\hk}(f)$ is ordinarily far harder
to compute than $\mu$. As a result~\eqref{eq:koksmahlawka}
can show us that MC can be outperformed but does
not provide a generally usable error estimate.

RQMC methods generate random points $\bsx_i$ that
simultaneously satisfy two properties:
\begin{compactenum}[\quad\bf1)]
\item $\bsx_i\sim\runif[0,1]^d$, and
\item $D_n^*(\bsx_1,\dots,\bsx_n)=\tilde O(n^{-1})$ almost surely.
\end{compactenum}

From the second property, $|\hat\mu-\mu|=\tilde O(n^{-1})$.
As a result the RMSE of RQMC is $\tilde O(n^{-2})$.
To get an approximate confidence interval from RQMC
we can form $R$ statistically independent RQMC estimates
$\hat\mu_1,\dots,\hat\mu_R$. Using the first property
above these IID estimates are unbiased and we can get
an asymptotic confidence interval for $\mu$ of the form
$$
\hat\mu \pm S t_{(R-1)}^{1-\alpha/2}/\sqrt{R}
$$
for
$$
\hat\mu = \frac1R\sum_{r=1}^R\hat\mu_r
\quad\text{and}\quad
S^2 = \frac1{R-1}\sum_{r=1}^R(\hat\mu_r-\hat\mu)^2.
$$
Here $S^2/R$ is an unbiased estimate of the RMSE
of the pooled estimate $\hat\mu$.
The confidence level $1-\alpha$ is obtained in
the limit as $R\to\infty$ \cite{naka:tuff:2024}
for fixed $n$.

In addition to getting an unbiased estimate of
the RQMC variance, randomization can provide
some other benefits.  First, some RQMC methods
can attain better accuracy than plain QMC.
Under sufficient smoothness, the scrambled 
digital sequences
in \cite{Owe95} attain RMSEs of $\tilde O(n^{-3/2})$
\cite{Owe97,Owe08a}. The same holds for the scramblings
of \cite{Mat98}. Those methods also have the
property that $\var(\hat\mu) = o(1/n)$ as $n\to\infty$
for any integrand with $\sigma^2<\infty$, and so their
efficiency with respect to MC becomes unbounded as $n\to\infty$.
Higher order digital nets \cite{Dic11a} can attain even
better convergence rates, though the rates are not
necessarily evident empirically when $d$ is moderately
large, which \cite{nuyens2010higher} attribute to numerical precision.
Furthermore, the condition of finite
variation in the sense of Hardy and Krause can be quite
strict. For example, when $d\ge2$, step discontinuities in the integrand
$f$ ordinarily make $V_{\hk}(f)$ infinite unless those discontinuities
are axis parallel \cite{Owe05a}. This makes QMC
unattractive for estimating integrals of binary
valued functions, while RQMC can still attain
$o(1/n)$ RMSE for them.

While there are numerous RQMC methods in use, we
will restrict ourselves here to scramblings of digital nets
and sequences, such as the ones of \cite{Sob67}.
The scramblings can be those of \cite{Owe95} or
\cite{Mat98}.


While RQMC methods yield very good accuracy, most
of the known confidence interval methods for them
supply only asymptotic confidence. That is ordinarily
asynptotice as $R\to\infty$ \cite{naka:tuff:2024}.
In some very limited settings
there are central limit theorems for $\hat\mu$
as $n\to\infty$ \cite{Loh01}.
Having the confidence statements be asymptotic in $R$
is unfortunate since the RMSE of $\hat\mu$ 
is proportional to $R^{-1/2}$ while the RMSE
vanishes at a faster rate in $n$.



\subsection{Hoeffding and Empirical Bernstein intervals}

We would like a confidence interval that is non-asymptotic,
meaning that the coverage guarantee (xx) holds for finite $n$.
This can often be obtained under parametric statistical
models in which the distribution of $f(\bsx)$  belongs
to a known finite dimensional family such as the Gaussian
distributions.  It is not desirable to make such assumptions
and so we prefer a nonparametric method with non-asymptotic
confidence.  

There is a theorem of Bahadur and Savage \cite{BahSav56} that
reveals sharp constrains on our ability to construct
nonparametric and non-asymptotic confidence
intervals.  Here is the description from \cite{err4qmc}:
\begin{quote}They consider a set $\cf$ of distributions on $\real$.
Letting $\mu(F)$ be $\e(Y)$ when $Y\sim F\in\real$, their conditions are:
\begin{compactenum}[\quad (i)]
\item For all $F\in\cf$, $\mu(F)$ exists and is finite.
\item For all $m\in\real$ there is $F\in\cf$ with $\mu(F)=m$.
\item $\cf$ is convex: if $F,G\in\cf$ and $0<\pi<1$,
  then $\pi F+(1-\pi)G\in \cf$.
\end{compactenum}
Then their Corollary 2 shows that a Borel set constructed
based on $Y_1,\dots,Y_N\stackrel{\mathrm{iid}}
\sim F$
that contains $\mu(F)$ with probability at least $1-\alpha$
also contains any other $m\in\real$ with probability at least $1-\alpha$.
More precisely: we can get a confidence set, but not a useful one.
They allow $N$ to be random so long as $\Pr(N<\infty)=1$.
\end{quote}

We can escape the restriction from Bahadur and
Savage by stipulating that every distribution in $\cf$
has support in a known interval $[a,b]$ of finite length.
This violates their clause (ii).
After a linear transformation we take that interval
to be $[0,1]$.

We will use $Y_i = f(\bsx_i)$ to represent 
the values in our integration methods, and 
set $\bar Y_m = (1/m)\sum_{i=1}^mY_i$.  
We use $m$ here in order to reserve $n$ for a
different sample size quantity later.
\art{[Probably change it to $R$ later because $m$ is
used a lot in the betting nomenclature.]}
We assume that $Y_i$ are IID with the same
distribution as $Y$ and that
$0\le Y\le1$ holds with probability one.
Then
\begin{align}\label{eq:hoeffding}
\bar Y_m \pm \sqrt{\frac{\log(2/\alpha)}{2m}}
\end{align}
for $0<\alpha<1$ is a $1-\alpha$ confidence interval for $\mu=\e(Y_i)$.  Equation~\eqref{eq:hoeffding} gives
the confidence interval of Hoeffding \cite{hoef:1963}.  The interval is not always
nested within $[0,1]$, but we can simply take its
intersection with $[0,1]$. That will preserve the confidence level.

While Hoeffding's interval is a valid confidence interval
for the mean of any distribution supported on $[0,1]$,
it can be quite conservative for some
of them, giving confidence over $1-\alpha$. This raises
the possibility of getting coverage $1-\alpha$ from
narrower intervals.  
One approach is to make use of 
$\sigma^2=\var(Y)$ if it is known. 
Hoeffding's confidence interval is a consequence of
Hoeffding's exponential probability bound in \cite{hoef:1963}.
Other exponential probability bounds take advantage of
a known value $\sigma^2=\var(Y)$ or even a sample estimate
of that variance.  Those bounds can be used to get
improved confidence intervals.

Using Bennett's inequality \cite{benn:1962}, 
Maurer and Pontil \cite{maurer2009empirical}
note that
$$
\e(Y) - \frac1m\sum_{i=1}^mY_i \le \sqrt{\frac{2\sigma^2\log(1/\delta)}m}
+ \frac{\log(1/\delta)}{3m}.
  $$
holds with probability at least $1-\delta$.
Taking $\delta =\alpha/2$ we get
\begin{align}\label{eq:bennettci}
\bar Y_m \pm \sqrt{\frac{2\sigma^2\log(2/\alpha)}m}
+ \frac{\log(2/\alpha)}{3m}
\end{align}
as a $1-\alpha$ confidence interval for $\mu$.
We will make use of the Bennett confidence
intervals~\eqref{eq:bennettci} for some
theoretical investigations. 

In most practical settings where $\mu$ is unknown,
$\sigma^2$ will also be unknown. Replacing $\sigma^2$
by $s^2$ in~\eqref{eq:bennettci} would not always
give a valid confidence interval.  
Theorem 4 of \cite{maurer2009empirical} show that
for $m\ge2$
\begin{align}\label{eq:eb}
    \bar Y_n\pm
\sqrt{\frac{2s^2\log(4/\alpha)}m}
+ \frac{7\log(4/\alpha)}{3(m-1)}.
\end{align}
is a $1-\alpha$ confidence interval for $\mu$.
This is called an empirical Bernstein confidence
interval named after an inequality of Bernstein
that is similar to Bennett's.  Equation~\eqref{eq:eb}
has improved constants compared to the first
empirical Bernstein inequality of 
\cite{audi:muno:szep:2007}. There is a further
improvement in \cite{WauRam24a}.



% tricky to find a detailed citation to Bernstein's work that I know is correct


\subsection{Betting methods}

Our interest in this problem was sparked in part by some recent
work by \cite{WauRam24a} on confidence intervals generated
by betting arguments.  They consider a more general setting
of confidence sequences that can then be specialized to
confidence intervals as needed.  
For random $Y_i\in[0,1]$ they assume
that $\e(Y_i\giv Y_1,\dots,Y_{i-1})=\mu$ for all $i\ge1$ 
without assuming
that the $Y_i$ are identically distributed or even that
they are independent. For $i=1$, the conditional expectation 
above is simply $\e(Y_1)=\mu$.

Suppose that the conditional mean is really $\mu$ and 
there is a null hypothesis $H_0(m)$ that
$\e(Y_i\giv Y_1,\dots, Y_{i-1})=m$. Then if $m\ne\mu$
somebody starting with a stake of \$1.00 can
make a series of bets against $H_0(m)$
as the $Y_i$ are revealed in order
and have an expected fortune that grows without bound.
Just prior to time $i$, the bettor picks a value
$\lambda_i\in(-1/(m-1),1/m)$. Let's ignore $m\in\{0,1\}$
although \cite{WauRam24a} include those values. The bettor's
capital at time $t\ge1$ is
$$
\ck_t(m)= \prod_{i=1}^t\bigl(1+\lambda_i(m)(Y_i-m)\bigr).
$$
A bet with $\lambda_i(m)>0$ pays off if $Y_i>m$ which
increases their capital. Choosing $\lambda_i<0$ is a 
way to bet that $Y_i<m$.

Section 4 of \cite{WauRam24a} notes that $\ck_t(\mu)$ is
a martingale, so that $\e(\ck_t(m))=1$ for all $t\ge1$.
There is no way for the bettor to pick bet sizes $\lambda_i(\mu)$
to make an expected profit.    From the martingale
property
$$
\Pr( \ck_t(\mu)\le 1/\alpha,\ \forall t\ge1)\ge1-\alpha.
$$
If we watch $\ck_t(\mu)$ indefinitely, then
there is at most probability $\alpha$ that it will
ever go above $1/\alpha$.  As a result we can get
a confidence sequence by retaining at time $t$,
all the values $m$ for 
which $\max_{1\le\tau\le t}\ck_t(m)\le1/\alpha$.
This idiom uses a hypothetically infinite ensemble of 
bettors to do this, but \cite{WauRam24a} give
algorithms for it, some of which we present below.

The other side of the coin is that there are good betting
strategies when $m\ne\mu$.  The bettor's expected fortune
will grow steadily when $m\ne\mu$. That is what one needs
to make a confidence sequence not just valid but also useful.

In addition to the capital process described
above, they have some other processes that
produce martingales. One of them produces confidence
sequences that are analogous to Hoeffding intervals
and another produces sequences analogous to the
empirical Bernstein intervals.  The latter produce
narrower intervals for large $t$, so we describe them.

Those confidence intervals make use of running sample moments
$$
\hat\mu_t = \frac{1/2+\sum_{i=1}^tY_i}{t+1}
\quad\text{and}\quad
\hat\sigma^2_t = \frac{1/4+\sum_{i=1}^t(Y_i-\hat\mu_i)^2}{t+1}
$$
with the means and standard deviations 
both biased slightly towards $1/2$.
The betting amounts are
$$
\lambda_t^\prpleb = \sqrt{\frac{2\log(2/\alpha)}{\hat\sigma^2_{t-1}t\log(1+t)}}\,\wedge c
$$
for some $c\in(0,1)$ with $c=1/2$ or $3/4$
given as reasonable defaults. The superscript on $\lambda_t$ 
is for `predictable plug-in empirical Bayes'.
Theorem 2 of \cite{WauRam24a} 
gives a $1-\alpha$ confidence sequence
$$
C_t^\prpleb = \frac{\sum_{i=1}^t\lambda_iY_i}{\sum_{i=1}^t\lambda_i}
\pm \frac{\log(2/\alpha)+\sum_{i=1}^t\nu_i\psi_{\eb}(\lambda_i)}{\sum_{i=1}^t\lambda_i},
$$
where
$$
\nu_i = 4(Y_i-\hat\mu_{i-1})^2
\quad\text{and}\quad \psi_{\eb}(\lambda)
= (-\log(1-\lambda)-\lambda)/4.
$$
The running intersection
$\cap_{1\le\tau\le t}C^\prpleb_\tau$
is also a $1-\alpha$ confidence sequence for~$\mu$ and
of course we can intersect any of these intervals with $[0,1]$.

These confidence sequences can be specialized to confidence
intervals when we have a fixed target sample size $R$
in mind.  For that case \cite{WauRam24a}
recommend
\begin{align}\label{eq:purplelambda}
\lambda_i^{\prpleb(R)} = \sqrt{\frac{2\log(2/\alpha)}{n\hat\sigma^2_{i-1}}}\,\wedge c,\quad i=1,\dots,R.
\end{align}
Later $R$ will be a number of RQMC replicates.
The above formula for $\lambda_i$ uses the ordering
of the data values and puts unequal weight on the
$R$ different $Y_i$. There is a permutation strategy
in \cite{WauRam24a} to treat the $Y_i$ values more
symmetrically, but they report that it makes little difference.

Equation (17) of \cite{WauRam24a} gives the
scaled half-width of the \prpleb{} confidence intervals as
$$
\sqrt{R}\biggl(\frac{\log(2/\alpha)+\sum_{i=1}^R\nu_i\psi_\eb(\lambda_i)}{\sum_{i=1}^R\lambda_i} \biggr)\toas \sigma\sqrt{2\log(2/\alpha)}
$$
as $R\to\infty$.  The corresponding limit for the empirical
Bernstein CIs of \cite{maurer2009empirical}
is $\sigma\sqrt{2\log(4/\alpha)}$.

% \art{[Scratch work]
% For IID data their equation (20) gives
% $$
% \e(W_t\giv Y_1,\dots,Y_{t-1}) \lesssim \frac{2\log(2/\alpha)+\sigma^2\sum_{i=1}^t\lambda^2_i/2}{\sum_{i=1}^t\lambda_i}.
% $$
% For $\lambda_i=\lambda^\star$ this becomes
% $$\frac{2\log(2/\alpha)+\sigma^2t{\lambda^\star}^2/2}{
% t\lambda^\star}.
% $$
% This is minimized at
% $$
% \lambda^\star = \sqrt{\frac{2\log(2/\alpha)}{\sigma^2 t}}
% $$
% yielding
% $$
% \frac{4\log(2/\alpha)}{t\lambda^\star}
% $$
% which unfortunately does not have two terms that we can
% optimize to make the best tradeoff
% }

\section{Asymptotic comparisons}

Known $\sigma$

Assumed rate for RQMC; examples with those rates; interval
of realistic rates

Maurer and Pontil

Ramdas and Waudby-Smith equation (20) and Table 1

Optimizing R+W-S is awkward

Optimizing Maurer and Pontil is easy

R+W-S show how their intervals relate to M+P's

Convergence rates for $n$ vs $N$

Slowly growing optimal $n$

Corresponding interval widths




\section{Empirical comparisons}
\subsection{Ridge functions}



\art{TODO: tidy up references.}
\bibliographystyle{plain}
\bibliography{FJHown25,FJH25,ebci4rqmc}

\end{document}