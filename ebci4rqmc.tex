\documentclass{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{paralist}
\usepackage{xspace}
\usepackage{ulem}

\newtheorem{theorem}{Theorem}

\newcommand{\art}[1]{\begingroup\color{blue}#1\endgroup}
\newcommand{\aadit}[1]{\begingroup\color{orange}#1\endgroup}
\newcommand{\fred}[1]{\begingroup\color{red}#1\endgroup}
\newcommand{\alexei}[1]{\begingroup\color{green}#1\endgroup}

% Lets make it look like writing on a blackboards
% not a 1970s typewriter!
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\emptyset}{\varnothing}

\input{FJHDef}

\newcommand{\real}{\mathbb{R}}
\newcommand{\tran}{\mathsf{\scriptsize T}}

\newcommand{\e}{\mathbb{E}}
\newcommand{\bsa}{\boldsymbol{a}}
\newcommand{\bsx}{\boldsymbol{x}}
\newcommand{\bsz}{\boldsymbol{z}}
\newcommand{\bsone}{\boldsymbol{1}}
\newcommand{\bszero}{\boldsymbol{0}}

\newcommand{\simiid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\toas}{\stackrel{\mathrm{a.s.}}{\to}}

\newcommand{\dunif}{\mathbb{U}}
\newcommand{\dnorm}{\mathcal{N}}

\newcommand{\giv}{\!\mid\!} % 


\newcommand{\prplh}{\text{PrPl-H}}
\newcommand{\prpleb}{\text{PrPl-EB}}
\newcommand{\prplpm}{\text{PrPl}\pm}
\newcommand{\eb}{\mathrm{E}}

% commands in lower case give less carpal trouble
\newcommand{\mc}{\mathrm{MC}}
\newcommand{\rqmc}{\mathrm{RQMC}}
\newcommand{\hk}{\mathrm{HK}}

\newcommand{\rd}{\,\mathrm{d}}
\newcommand{\phz}{\phantom{0}}

\newcommand{\jmp}{\mathrm{jump}}
\newcommand{\knk}{\mathrm{kink}}
\newcommand{\smo}{\mathrm{smooth}}
\newcommand{\fin}{\mathrm{finan.}}

\title{Empirical Bernstein and betting confidence intervals for randomized quasi-Monte Carlo}
\date{March 2025}
\author{List of authors}
\begin{document}
\maketitle
\begin{abstract}
There has been some recent progress in constructing empirical Bernstein
confidence intervals for the mean of a bounded random variable.
Randomized quasi-Monte Carlo (RQMC) methods estimate the mean of a random
variable using some equidistributed sample points in the unit cube
but it can be difficult to get confidence intervals for RQMC
estimates.  In this paper we study empirical Bernstein confidence
intervals for RQMC estimates.  We suppose that
the RQMC variance is $\tilde O(N^{-\theta})$\fred{, i.e., neglecting powers of $\log (n)$,} where
theory tells us that $1<\theta\le3$ \fred{[Do we include $1$, since for $L_2$ integrands that is what we have?]} 
\art{[Good point. For $L_2$, we know that the variance is $o(1/n)$. That in itself
does not mean that it is $\tilde O(n^{-\theta})$ for some $\theta>1$, so I think
we need to include $\theta=1$. It might be possible to construct an adversarially
chosen set of Fourier-Walsh coefficients to make the variance decay as $O(\log(n)/n)$ or worse.]}
is a reasonable
range with more favorable integrands having larger
$\theta$. When there are $N$ integrand evaluations
partitioned into $R$ independent replicates each using $n=N/R$
RQMC points each, we show that the optimal point set size $n$ grows
as $N^{1/(1+\theta)}$. The resulting intervals have
a width that is $\tilde O(N^{-\theta/(1+\theta)})$. 
This is  much narrower
than we would get under plain Monte Carlo (MC) sampling
but also a large multiple of \fred{$\tilde O(N^{(\theta - 1)/(1+\theta)})$ }the RQMC standard error $\tilde O(N^{-\theta/2})$ in those cases where $\theta$ is largest.
\end{abstract}

\aadit{Are we going to talk about the Hedged Betting CI as I did simulations on those as well and they are likely the tightest non-asymptotic CIs?}
\art{Yes, we can put those in as a numerical comparison, even if we don't have theory for them.  From memory, I think they come out slightly narrower
than the intervals we study but maybe asymptotically equivalent.}

\art{Earlier we discussed a comparison with methods that use a
bounded kurtosis. There would be some difficulties including that
in this paper.  That method starts with a fixed error bound $\epsilon$. So
it is more comparable to confidence sequences that we have not
studied.}

\section{Introduction}

We study the combination of randomized quasi-Monte
Carlo (RQMC) integration \fred{[We talk about integration, but then the confidence intervals are typically about random variables.  Should we specify early that we want CIs for $\mu = \mathbb{E}(Y)$, where $Y = f(\boldsymbol{X})$ and $\boldsymbol{X} \sim \mathbb{U}[0,1]^d$, so $\mu = \int_{[0,1]^d} f(\boldsymbol{x}) \, \mathrm{d} \boldsymbol{x}$, rather than wait to p.\ 4?]} 
with some non-asymptotic\fred{---valid for finite samples---}confidence interval methods.
RQMC has significant accuracy  benefits over plain Monte Carlo (that
we outline below), but so far most of the confidence intervals based on
RQMC estimates have had justifications that were asymptotic
in their sample size \fred{\cite{LEcEtal24a} or made assumptions about the integrand that might be impossible to verify in practice \cite{HicJim16a,JimHic16a}}.  Empirical Bernstein and some related betting
methods (described below) give finite sample assurances for bounded integrands.  Specifically, if it is known that the integrand values must lie
between $0$ and $1$, then those methods can construct a confidence
interval that has at least $1-\alpha$ probability to contain
the integral's value.  This confidence level holds for any integrand
satisfying the given bounds.  Any other finite interval $[a,b]$ known
to contain the integrand can, after linear transformation
be assumed to be the interval $[0,1]$.

The context for our work is as follows.
Many scientific problems require the numerical 
evaluation of multidimensional
integrals. When the dimension is high enough then classical \fred{tensor product cubature} rules 
like those in \cite{DavRab84} become too expensive to use. It is then
common to use plain Monte Carlo (MC) methods instead.  MC
methods based on random sampling typically have root mean
squared errors (RMSEs) of $O(n^{-1/2})$ given $n$
function evaluations.  This rate is slow but it is the
same in any dimension.  MC also allows one to use statistical
methods to quantify the uncertainty in the estimated
integral.  Quasi-Monte Carlo (QMC) 
methods (e.g.,  \cite{DicPil10a} and \cite{Nie92}) are deterministic
sampling strategies that under a bounded variation assumption
produce integral estimates with error $\tilde O(n^{-1})$ where
$\tilde O(\cdot)$ means we neglect powers of $\log(n)$.
Plain QMC methods are deterministic and so they lose the
uncertainty quantification advantage of MC. RQMC methods, 
surveyed in \cite{LEcLem02a}, allow one
one to make independent replicates of a statistically
unbiased QMC method to support variance estimates and
asymptotic confidence intervals.  RQMC methods can also
improve on the convergence rate of QMC methods obtaining
an RMSE of $\tilde O(n^{-3/2})$ under a smoothness assumption
\cite{Owe97,Owe08a} on the integrand.  We study RQMC
method assuming that their variance is $\tilde O(n^{-\theta})$.
We focus on two main cases, a bounded variation case with
$\theta=2$ and a smooth case with $\theta=3$.

Most confidence interval methods for RQMC are asymptotic
with coverage levels that converge to their desired level,
such as 95\%, as the amount of sampling increases.  The GAIL \fred{\cite{Gail_ug} and qmcpy \cite{QMCPy2020a} } libraries provides an exception, so long as one can
provide some outside information on the integrand.  This could
be an assumption about a kurtosis quantity or it could be
an assumption about the way  in which Fourier\fred{\sout{-Walsh}[Fourier Walsh for nets and Fourier complex exponential for lattice]} coefficients
decay.  Our goal here is to study RQMC confidence intervals
under the simple assumption that the integrand is bounded between
zero and one.  The advantage of this assumption over others is 
that in some applications we have no doubt as to whether the
assumption holds. We can then get finite sample confidence
intervals with coverage at least 95\% or some other desired level.

The non-asymptotic confidence intervals that we emphasize are the
recently developed predictable plug-in empirical Bernstein
confidence intervals of \cite{WauRam24a}. They are derived
from some infinite confidence sequences and we believe that
they provide the narrowest confidence intervals among
currently available methods.

Section~\ref{sec:notation} gives our notation
and some definitions and background on quasi-Monte
Carlo, randomized quasi-Monte Carlo, empirical Bernstein
intervals and the recent betting-based confidence
intervals of \cite{WauRam24a}. In Section \ref{sec:asymptotic}
we show how splitting $N$ observations into $R=N/n$ independent
replicates of $n$ RQMC point sets can give narrower
empirical Bernstein confidence intervals.  The extent of the
narrowing depends on the RQMC convergence rate which
varies from problem to problem. Perhaps surprisingly,
the more effective RQMC is, the smaller the optimal value
of $n$ becomes. We find $n=O(N^{1/(1+\theta)})$ \fred{Do we need $\tilde O$ here and below?} gives
the narrowest intervals.  Those intervals are narrower
than MC-based intervals by a factor of $O(N^{(1-\theta)/(2+2\theta)})$.
The intervals themselves have widths that are
$O(N^{-\theta/(1+\theta)})$\fred{, which makes them $\tilde O(N^{(\theta_1)/(1+\theta)})$ thicker than the root mean squared RQMC error}. Section~\ref{sec:finite} makes
some computational investigations.  As predicted by the theory
the best values of $n$ to use grow slowly with $N$.
Section~\ref{sec:discussion} has some final comments
and discussion.


\section{Background and notation}\label{sec:notation}

We begin with some notation.
We use $1\{E\}$ to denote a quantity that equals $1$
when expression $E$ holds and is zero otherwise.
The vectors $\bszero$ and $\bsone$ have all components
equal to $0$ (respectively $1$) with a dimension
given by context.

\subsection{Intervals and martingales}

For $0\le\alpha\le 1$, a $1-\alpha$ confidence interval for a 
parameter $\mu$ is a pair of random quantities $A$ and $B$ with
$$
\Pr( A\le \mu\le B)\ge 1-\alpha.
$$
A confidence interval is strict when the right hand
side above is an equality.
An asymptotic confidence interval is a sequence $(A_n,B_n)$
of random quantities for which
$$
\lim_{n\to\infty}\Pr( A_n\le \mu\le B_n)\ge 1-\alpha.
$$
It is common for asymptotic confidence intervals to
be strict, meaning that this limit equals $1-\alpha$.
The best known example is Student's $t$ confidence
interval for the mean of a distribution with finite
variance, based on a random sample 
from that distribution and justified by the
central limit theorem.  It is asymptotically strict.
%Fred suggests a paragraph break

The random sequence $(A_n,B_n)$ is a $1-\alpha$ confidence sequence
for $\mu$ if
$$
\Pr( A_n \le \mu \le B_n,\ \forall n\ge1)\ge 1-\alpha.
$$
Confidence sequences allow us to select a stopping time
$\nu$ based on $\{(A_n,B_n)\mid n\le\nu\}$ and have
at least $1-\alpha$ confidence that $A_\nu\le \mu\le B_\nu$.
We can take the limits to be $\max_{n\le \nu}A_n$
and $\min_{n\le\nu}B_n$.

Martingale theory is one of the main tools for
obtaining confidence sequences.
If random variables $S_i$ for $i\ge1$
satisfy $\e(S_n\giv S_1,\dots,S_{n-1})=S_{n-1}$
then $(S_i)_{i\ge1}$ is a martingale.
If $\e(S_n\giv S_1,\dots,S_{n-1})\le S_{n-1}$
then $(S_i)_{i\ge1}$ is a supermartingale.
Ville's inequality is that for any $\eta>0$
and any nonnegative supermartingale $(S_i)_{i\ge1}$
$$
\Pr\biggl( \sup_{n\ge1}S_n\ge \eta\biggr)\le\frac{\e(S_1)}\eta.
$$
The confidence sequences from \cite{WauRam24a} are derived
by applying Ville's inequality \cite{vill:1939} to nonnegative supermartingales.
\art{[TODO: check whether they're actually using a submartingale argument.]}


The problem we consider is to approximate the finite-dimensional
integral
$$\mu = \int_{[0,1]^d}f(\bsx)\rd\bsx \fred{= \mathbb{E}[f(\boldsymbol{X})], \quad \text{where } \boldsymbol{X} \sim \dunif[0,1]^d}.$$
In MC sampling we take $\bsx_i\simiid \dunif[0,1]^d$ and
estimate $\mu$ by
$$
\hat\mu = \frac1n\sum_{i=1}^nf(\bsx_i).
$$
Let $\sigma^2=\var(f(\bsx))$ for $\bsx\sim\dunif[0,1]^d$.
We assume that $0<\sigma^2<\infty$. In that case the RMSE
of $\hat\mu$ is $\sigma/\sqrt{n}$. For $n\ge2$,
$$s^2 =\frac1{n-1}\sum_{i=1}^n(f(\bsx_i)-\hat\mu)^2$$
is an unbiased estimate of $\sigma^2$. Furthermore
$$
\lim_{n\to\infty}\Pr\Bigl( \sqrt{n}\frac{\hat\mu-\mu}s\le t^{1-\alpha}_{(n-1)}\Bigr) = 1-\alpha
$$
where $t^{1-\alpha}_{(n-1)}$ is the $1-\alpha$ quantile of
Student's $t$ distribution on $n-1$ degrees of freedom.
Then $\hat\mu \pm st^{1-\alpha/2}/\sqrt{n}$ is an
asymptotic $1-\alpha$ confidence interval for~$\mu$.

\subsection{QMC and RQMC}

QMC sampling replaces random points $\bsx_i$ by 
deterministic points that more evenly sample the
unit cube.  There are many ways to quantify that 
property but the most widely used one is the
star discrepancy
$$
D_n^* = D_n^*(\bsx_1,\dots,\bsx_n)=\sup_{\bsa\in[0,1]^d}
\Bigl| \frac1n\sum_{i=1}^n1\{\bsx_i\in[\bszero,\bsa)\}
-\prod_{j=1}^da_j\Bigr|.
$$
Small values of $D_n^*$ show that all anchored boxes $[\bszero,\bsa)$
have very nearly the desired proportion of the $n$ points,
which is the volume of $[\bszero,\bsa)$.
It is possible to choose points $\bsx_1,\dots,\bsx_n$ so
that $D_n^*=\tilde O(n^{-1})$ while plain MC points
have $D_n^*=\tilde O(n^{-1/2})$ \cite{Nie92}.

The improvement in star discrepancy yields an improvement
in integration error via the Koksma-Hlawka inequality \cite{Hic04a}:
\begin{align}\label{eq:koksmahlawka}
|\hat\mu-\mu|\le D_n^*(\bsx_1,\dots,\bsx_n)\times V_{\hk}(f).
\end{align}
As described above $D_n^*$ is a measure of how non-uniform
the points $\bsx_i$ are. The new factor $V_{\hk}(f)$ is the
total variation of $f$ in the sense of Hardy and Krause.
See \cite{Owe05a}. It follows from~\eqref{eq:koksmahlawka} 
that we can get $|\hat\mu-\mu|=\tilde O(n^{-1})$
using QMC. We will refer to this as the BVHK rate.

If we knew $D_n^*$ and $V_\hk(f)$, then~\eqref{eq:koksmahlawka} would provide a
perfect quantification of our uncertainty about $\mu$.
It would be non-asymptotic and even non-probabilistic.
Unfortunately, $D_n^*$ is generally very expensive
to obtain and $V_{\hk}(f)$ is ordinarily far harder
to compute than $\mu$. As a result~\eqref{eq:koksmahlawka}
can show us that MC can be outperformed but does
not provide a generally usable error estimate.

RQMC methods generate random points $\bsx_i$ that
simultaneously satisfy two properties:
\begin{compactenum}[\quad\bf1)]
\item $\bsx_i\sim\dunif[0,1]^d$, and
\item $D_n^*(\bsx_1,\dots,\bsx_n)=\tilde O(n^{-1})$ almost surely.
\end{compactenum}

From the second property, $|\hat\mu-\mu|=\tilde O(n^{-1})$.
As a result the \fred{\sout{R}}MSE of RQMC is $\tilde O(n^{-2})$
in the BVHK case.
To get an approximate confidence interval from RQMC
we can form $R$ statistically independent RQMC estimates
$\hat\mu_1,\dots,\hat\mu_R$. Using the first property
above these IID estimates are unbiased and we can get
an asymptotic confidence interval for $\mu$ of the form
$$
\hat\mu \pm S t_{(R-1)}^{1-\alpha/2}/\sqrt{R}
$$
for
$$
\hat\mu = \frac1R\sum_{r=1}^R\hat\mu_r
\quad\text{and}\quad
S^2 = \frac1{R-1}\sum_{r=1}^R(\hat\mu_r-\hat\mu)^2.
$$
Here $S^2/R$ is an unbiased estimate of the RMSE
of the pooled estimate $\hat\mu$.
The confidence level $1-\alpha$ is obtained in
the limit as $R\to\infty$ \cite{naka:tuff:2024}
for fixed $n$.

In addition to getting an unbiased estimate of
the RQMC variance, randomization can provide
some other benefits.  First, some RQMC methods
can attain better accuracy than plain QMC.
Under sufficient smoothness, the scrambled  digital sequences
in \cite{Owe95} attain RMSEs of $\tilde O(n^{-3/2})$
\cite{Owe97,Owe08a}. The same holds for the scramblings
of \cite{Mat98}. We will refer to this as the `smooth case'.
Higher order digital nets \cite{Dic11a} can attain even
better convergence rates, though the rates are not
necessarily evident empirically when $d$ is moderately
large, which \cite{nuyens2010higher} attribute to numerical precision.

Those RQMC methods have further
useful properties.  First,  $\var(\hat\mu) = o(1/n)$ as $n\to\infty$
for any integrand with $\sigma^2<\infty$, and so their
efficiency with respect to MC becomes unbounded as $n\to\infty$.
Furthermore, the condition of finite
variation in the sense of Hardy and Krause can be quite
strict. For example, when $d\ge2$, step discontinuities in the integrand
$f$ ordinarily make $V_{\hk}(f)$ infinite unless those discontinuities
are axis parallel \cite{Owe05a}. This makes QMC
unattractive for estimating integrals of binary
valued functions, while RQMC will still attain
$o(1/n)$ RMSE for them so long as the integrand
is in $L^2$.

While there are numerous RQMC methods in use, we
will restrict ourselves here to scramblings of digital nets
and sequences, such as the ones of \cite{Sob67}.
The scramblings can be those of \cite{Owe95} or
\cite{Mat98}.


While RQMC methods yield very good accuracy, most
of the known confidence interval methods for them
supply only asymptotic confidence. That is ordinarily
asynptotice as $R\to\infty$ \cite{naka:tuff:2024}.
In some very limited settings
there are central limit theorems for $\hat\mu$
as $n\to\infty$ \cite{Loh01}.
Having the confidence statements be asymptotic in $R$
is unfortunate since the RMSE of $\hat\mu$ 
is proportional to $R^{-1/2}$ while the RMSE
vanishes at a faster rate in $n$.



\subsection{Hoeffding and Empirical Bernstein intervals}

We would like a confidence interval that is non-asymptotic,
meaning that the coverage guarantee (xx) holds for finite $n$.
This can often be obtained under parametric statistical
models in which the distribution of $f(\bsx)$  belongs
to a known finite dimensional family such as the Gaussian
distributions.  It is not desirable to make such assumptions
and so we prefer a nonparametric method with non-asymptotic
confidence.  

There is a theorem of Bahadur and Savage \cite{BahSav56} that
reveals sharp constrains on our ability to construct
nonparametric and non-asymptotic confidence
intervals.  Here is the description from \cite{err4qmc}:
\begin{quote}They consider a set $\cf$ of distributions on $\real$.
Letting $\mu(F)$ be $\e(Y)$ when $Y\sim F\in\real$, their conditions are:
\begin{compactenum}[\quad (i)]
\item For all $F\in\cf$, $\mu(F)$ exists and is finite.
\item For all $m\in\real$ there is $F\in\cf$ with $\mu(F)=m$.
\item $\cf$ is convex: if $F,G\in\cf$ and $0<\pi<1$,
  then $\pi F+(1-\pi)G\in \cf$.
\end{compactenum}
Then their Corollary 2 shows that a Borel set constructed
based on $Y_1,\dots,Y_N\stackrel{\mathrm{iid}}
\sim F$
that contains $\mu(F)$ with probability at least $1-\alpha$
also contains any other $m\in\real$ with probability at least $1-\alpha$.
More precisely: we can get a confidence set, but not a useful one.
They allow $N$ to be random so long as $\Pr(N<\infty)=1$.
\end{quote}

We can escape the restriction from Bahadur and
Savage by stipulating that every distribution in $\cf$
has support in a known interval $[a,b]$ of finite length.
This violates their clause (ii).
After a linear transformation we take that interval
to be $[0,1]$.

We will use $Y_i = f(\bsx_i)$ to represent 
the values in our integration methods, and 
set $\bar Y_m = (1/m)\sum_{i=1}^mY_i$.  
We use $m$ here in order to reserve $n$ for a
different sample size quantity later.
\art{[Probably change it to $R$ later because $m$ is
used a lot in the betting nomenclature.]}
We assume that $Y_i$ are IID with the same
distribution as $Y$ and that
$0\le Y\le1$ holds with probability one.
Then
\begin{align}\label{eq:hoeffding}
\bar Y_m \pm \sqrt{\frac{\log(2/\alpha)}{2m}}
\end{align}
for $0<\alpha<1$ is a $1-\alpha$ confidence interval for $\mu=\e(Y_i)$.  Equation~\eqref{eq:hoeffding} gives
the confidence interval of Hoeffding \cite{hoef:1963}.  The interval is not always
nested within $[0,1]$, but we can simply take its
intersection with $[0,1]$. That will preserve the confidence level.

While Hoeffding's interval is a valid confidence interval
for the mean of any distribution supported on $[0,1]$,
it can be quite conservative for some
of them, giving confidence over $1-\alpha$. This raises
the possibility of getting coverage $1-\alpha$ from
narrower intervals.  
One approach is to make use of 
$\sigma^2=\var(Y)$ if it is known. 
Hoeffding's confidence interval is a consequence of
Hoeffding's exponential probability bound in \cite{hoef:1963}.
Other exponential probability bounds take advantage of
a known value $\sigma^2=\var(Y)$ or even a sample estimate
of that variance.  Those bounds can be used to get
improved confidence intervals.

\art{[Bennett's inequality in the form we use
does not appear in Bennett \cite{benn:1962}.]
Theorem 3 of Maurer and Pontil \cite{maurer2009empirical}
is that
$$
\e(Y) - \frac1m\sum_{i=1}^mY_i \le \sqrt{\frac{2\sigma^2\log(2/\alpha)}m}
+ \frac{\log(2/\alpha)}{3m}.
  $$
holds with probability at least $1-\delta$.
They call this Bennett's inequality, because it derives
from an inequality in \cite{benn:1962}.}
Taking $\delta =\alpha/2$ we get
\begin{align}\label{eq:bennettci}
\bar Y_m \pm \sqrt{\frac{2\sigma^2\log(2/\alpha)}m}
+ \frac{\log(2/\alpha)}{3m}
\end{align}
as a $1-\alpha$ confidence interval for $\mu$.
We will make use of the Bennett confidence
intervals~\eqref{eq:bennettci} for some
theoretical investigations. 

In most practical settings where $\mu$ is unknown,
$\sigma^2$ will also be unknown. Replacing $\sigma^2$
by $s^2$ in~\eqref{eq:bennettci} would not always
give a valid confidence interval.  
Theorem 4 of \cite{maurer2009empirical} show that
for $m\ge2$
\begin{align}\label{eq:eb}
    \bar Y_n\pm
\sqrt{\frac{2s^2\log(4/\alpha)}m}
+ \frac{7\log(4/\alpha)}{3(m-1)}.
\end{align}
is a $1-\alpha$ confidence interval for $\mu$.
This is called an empirical Bernstein confidence
interval named after an inequality of Bernstein
that is similar to Bennett's.  We see that both terms
on the right hand side of~\eqref{eq:eb} are larger
than the ones in the Bennett interval~\eqref{eq:bennettci}.
Their interval has improved constants compared to the original
empirical Bernstein inequality from
\cite{audi:muno:szep:2007}. Some betting intervals
in \cite{WauRam24a} provide a further improvement.



% tricky to find a detailed citation to Bernstein's work that I know is correct


\subsection{Betting methods}

Our interest in this problem was sparked in part by some recent
work by \cite{WauRam24a} on confidence intervals generated
by betting arguments.  They consider a more general setting
of confidence sequences that can then be specialized to
confidence intervals as needed.  
For random $Y_i\in[0,1]$ they assume
that $\e(Y_i\giv Y_1,\dots,Y_{i-1})=\mu$ for all $i\ge1$ 
without assuming
that the $Y_i$ are identically distributed or even that
they are independent. For $i=1$, the conditional expectation 
above is simply $\e(Y_1)=\mu$.

Suppose that the conditional mean is really $\mu$ and 
there is a null hypothesis $H_0(m)$ that
$\e(Y_i\giv Y_1,\dots, Y_{i-1})=m$. Then if $m\ne\mu$
somebody starting with a stake of \$1.00 can
make a series of bets against $H_0(m)$
as the $Y_i$ are revealed in order
and have an expected fortune that grows without bound.
Just prior to time $i$, the bettor picks a value
$\lambda_i\in(-1/(m-1),1/m)$. Let's ignore $m\in\{0,1\}$
although \cite{WauRam24a} include those values. The bettor's
capital at time $t\ge1$ is
$$
\ck_t(m)= \prod_{i=1}^t\bigl(1+\lambda_i(m)(Y_i-m)\bigr).
$$
A bet with $\lambda_i(m)>0$ pays off if $Y_i>m$ which
increases their capital. Choosing $\lambda_i<0$ is a 
way to bet that $Y_i<m$.

Section 4 of \cite{WauRam24a} notes that $\ck_t(\mu)$ is
a martingale, so that $\e(\ck_t(\mu))=1$ for all $t\ge1$.
There is no way for the bettor to pick bet sizes $\lambda_i(\mu)$
to make an expected profit.    From the martingale
property
$$
\Pr( \ck_t(\mu)\le 1/\alpha,\ \forall t\ge1)\ge1-\alpha.
$$
If we watch $\ck_t(\mu)$ indefinitely, then
there is at most probability $\alpha$ that it will
ever go above $1/\alpha$.  As a result we can get
a confidence sequence by retaining at time $t$,
all the values $m$ for 
which $\max_{1\le\tau\le t}\ck_t(m)\le1/\alpha$.
This idiom uses a hypothetically infinite ensemble of 
bettors to do this, but \cite{WauRam24a} give
algorithms for it, some of which we present below.

The other side of the coin is that there are good betting
strategies when $m\ne\mu$.  The bettor's expected fortune
will grow steadily when $m\ne\mu$. That is what one needs
to make a confidence sequence not just valid but also useful
in having a width that converges to zero with increased sampling.

In addition to the capital process described
above, they have some other processes that
produce different martingales. One of them produces confidence
sequences that are analogous to Hoeffding intervals
and another produces sequences analogous to the
empirical Bernstein intervals.  We focus on the analogues 
of empirical Bernstein intervals because for large $t$
they are narrower than the analogues of the Hoeffding intervals.


Those confidence intervals make use of running sample moments
$$
\hat\mu_t = \frac{1/2+\sum_{i=1}^tY_i}{t+1}
\quad\text{and}\quad
\hat\sigma^2_t = \frac{1/4+\sum_{i=1}^t(Y_i-\hat\mu_i)^2}{t+1}
$$
with the means and standard deviations 
both biased slightly towards $1/2$.
The betting amounts are
$$
\lambda_t^\prpleb = \sqrt{\frac{2\log(2/\alpha)}{\hat\sigma^2_{t-1}t\log(1+t)}}\,\wedge c
$$
for some $c\in(0,1)$ with $c=1/2$ or $3/4$
given as reasonable defaults. The superscript on $\lambda_t$ 
is for `predictable plug-in empirical Bayes'.
Theorem 2 of \cite{WauRam24a} 
gives a $1-\alpha$ confidence sequence
$$
C_t^\prpleb = \frac{\sum_{i=1}^t\lambda_iY_i}{\sum_{i=1}^t\lambda_i}
\pm \frac{\log(2/\alpha)+\sum_{i=1}^t\nu_i\psi_{\eb}(\lambda_i)}{\sum_{i=1}^t\lambda_i},
$$
where
$$
\nu_i = 4(Y_i-\hat\mu_{i-1})^2
\quad\text{and}\quad \psi_{\eb}(\lambda)
= (-\log(1-\lambda)-\lambda)/4.
$$
The running intersection
$\cap_{1\le\tau\le t}C^\prpleb_\tau$
is also a $1-\alpha$ confidence sequence for~$\mu$ and
of course we can intersect any of these intervals with $[0,1]$.

These confidence sequences can be specialized to confidence
intervals when we have a fixed target sample size $R$
in mind.  For that case \cite{WauRam24a}
recommend
\begin{align}\label{eq:purplelambda}
\lambda_i^{\prpleb(R)} = \sqrt{\frac{2\log(2/\alpha)}{n\hat\sigma^2_{i-1}}}\,\wedge c,\quad i=1,\dots,R.
\end{align}
Later $R$ will be a number of RQMC replicates.
The above formula for $\lambda_i$ uses the ordering
of the data values and puts unequal weight on the
$R$ different $Y_i$. There is a permutation strategy
in \cite{WauRam24a} to treat the $Y_i$ values more
symmetrically, but they report that it makes little difference.

Equation (17) of \cite{WauRam24a} gives the
scaled half-width of the \prpleb{} confidence intervals as
$$
\sqrt{R}\biggl(\frac{\log(2/\alpha)+\sum_{i=1}^R\nu_i\psi_\eb(\lambda_i)}{\sum_{i=1}^R\lambda_i} \biggr)\toas \sigma\sqrt{2\log(2/\alpha)}
$$
as $R\to\infty$.  The corresponding limit for the empirical
Bernstein CIs of \cite{maurer2009empirical}
is $\sigma\sqrt{2\log(4/\alpha)}$.

% \art{[Scratch work]
% For IID data their equation (20) gives
% $$
% \e(W_t\giv Y_1,\dots,Y_{t-1}) \lesssim \frac{2\log(2/\alpha)+\sigma^2\sum_{i=1}^t\lambda^2_i/2}{\sum_{i=1}^t\lambda_i}.
% $$
% For $\lambda_i=\lambda^\star$ this becomes
% $$\frac{2\log(2/\alpha)+\sigma^2t{\lambda^\star}^2/2}{
% t\lambda^\star}.
% $$
% This is minimized at
% $$
% \lambda^\star = \sqrt{\frac{2\log(2/\alpha)}{\sigma^2 t}}
% $$
% yielding
% $$
% \frac{4\log(2/\alpha)}{t\lambda^\star}
% $$
% which unfortunately does not have two terms that we can
% optimize to make the best tradeoff
% }

\section{Asymptotic confidence interval comparisons}\label{sec:asymptotic}

Our confidence intervals will use $R$ IID replicates that each
use $n$ RQMC points in $[0,1]^d$.  That takes $N=nR$
evaluations of $f$. 
Here we study how to get the narrowest confidence
intervals with that budget of $N$ function evaluations.

We take 
$$Y_i=\frac1n\sum_{k=1}^nf(\bsx_{i,k})$$
where for each $i$, $\bsx_{i,1},\dots,\bsx_{i,n}$ is an RQMC
point set. The $Y_i$ are IID when we use independent
scrambles for all $R$ point sets. Let $\sigma^2_n = \var(Y_i)$.
We know that $\sigma^2_n = o(n^{-1}).$
For the BVHK case, $\sigma^2_n=\tilde O(n^{-2})$.  
For the smooth case,
$\sigma^2_n=\tilde O(n^{-3})$.  
The tilde in $\tilde O$ hides powers of $\log(n)$.
Those powers are present in error bounds for adversarially
chosen integrands but do not seem to come up in
real problems \cite{schl:2002,wherearethelogs}.
We will consider working
models of the form
\begin{align}\label{eq:themodel}
\sigma^2_n = \sigma^2_0n^{-\theta}.
\end{align}
\art{[Now using $\theta$ for variance not standard deviation.
The reason is that our reference values are more memorable
for the variance.]}
These provide insight even though the actual variance is not
necessarily any power of $n$ and it is more convenient
than using the idiom $O(n^{-\theta+\epsilon})$ here.
We know that taking $\theta=1$ underestimates
the quality of RQMC. The smooth case with $\theta = 3$ is optimistic
while the BVHK case with $\theta=2$ is intermediate.

If $R\to\infty$ for fixed $n$, then for the predictable
plug-in empirical Bayes intervals
\begin{align}\label{eq:purplewidth}
\sqrt{R}W^\prpleb\to \sigma_n\sqrt{2\log(2/\alpha)}.
\end{align}
The interval width is thus $O(n^{-\theta/2}R^{-1/2})$.
For a fixed product $N=nR$, this is narrowest at $R=1$
and $n=N$, but of course that is infeasible and also
that argument ignores the fact that~\eqref{eq:purplewidth}
is based on a limiting argument as $R\to\infty$.

To study the tradeoff between $n$ and $R$, we use Bennett's 
inequality which gives a half width of
\begin{align}\label{eq:bennettwidth}
W(n,R)=\sqrt{\frac{2\sigma_n^2\log(2/\alpha)}R}
+ \frac{\log(2/\alpha)}{3R}
\end{align}
when using $R$ IID observations that each have variance
$\sigma^2_n$. An oracle that knew $\sigma^2_n$ could
use this formula to select the best $n$ and $R$
for a confidence interval subject to a constraint $nR=N$.
The predictable plug-in empirical Bernstein confidence interval
from \cite{WauRam24a} does not assume a known variance
for the $Y_i$ and it does not handle all $Y_i$ values
symmetrically because they use the ordering.  However
that interval still attains precisely the same asymptotic limiting width
that an oracle would get from Bennett's formula. 

Next, we investigate the oracle's choice of $n$ and $R$
to minimize the half-width subject to $nR=N$. 
While $n$ and $R$ both have to be positive integers
with product $N$, we will first relax the problem
to a continuous variable $n$ with $R=N/n$.  After that, we
discuss integer solutions.

\begin{theorem}\label{thm:goodn}
Let $\sigma^2_n$ follow~\eqref{eq:themodel} and choose $N>0$.
If $\theta=1$, then the minimizer of~\eqref{eq:bennettwidth} 
over $n\in[1,\infty)$ is $n_*=1$.
If $\theta>1$ then the minimizer of~\eqref{eq:bennettwidth}
over $n\in(0,\infty)$ is 
\begin{align}\label{eq:goodn}
n_*& 
%= \left(\frac{3(\theta-1)\sigma_0
%\sqrt{N}}{\sqrt{2\log(2/\alpha)}}\right)^{2/(\theta+1)}
 = 
\left(\frac{9(\theta-1)^2\,\sigma_0^2
N}{2\log(2/\alpha)}\right)^{1/(\theta+1)}.
\end{align}\end{theorem}
\art{[I revised this to have exponent $1/(\theta+1)$
that seems to pop up in a lot of places and it is the
exponent of $N$ so that seems like the right choice. It also
leaves one less $\sqrt{\ }$ sign and I like using $\sigma_0^2$ more
than using $\sigma_0$. The prior expression is commented out.]} 

\art{[We also see that $N$ and $\sigma^2_0$ only appear
as their product.  We know that $0\le\sigma_0^2\le1/4$ must hold because
the most variance we can have for a random variable bounded between
$0$ and $1$ is $1/4$. So we know that $N\sigma_0^2\le N/4$. I use that
below in some guidance on choosing $n$. For $N=1024$, we would
always pick $n<7$. The best power of 2 might still be $8$.]}

\aadit{For the optimum $n_*$, shouldn't it be $(3(\theta - 1))^2$ instead of it being under the square root?}
\art{Thanks, I've changed it, but have not yet propagated that to the
next theorem.}

\begin{proof}
Under the model~\eqref{eq:themodel}, with $R=N/n$
the oracle's half width is
$$
W(n)=\sigma_0n^{(1-\theta)/2}N^{-1/2}
\sqrt{2\log(2/\alpha)}
+\frac{\log(2/\alpha)}{3}\frac{n}N
$$
If $\theta=1$, then $W(n)$ is minimized over $[1,\infty)$ at $n=1$.

For $\theta>1$, the function $W(n)$ is a convex function of $n$.
It will have a unique minimum over $(0,\infty)$ at
some $n_*$ where $W'(n_*)=0$. Now
\begin{align*}
W'(n)=\frac{1-\theta}2\frac{\sigma_0}{\sqrt{N}}n^{-(1+\theta)/2}
\sqrt{2\log(2/\alpha)}  + \frac{\log(2/\alpha)}{3N}.
\end{align*}
This vanishes at 
\begin{align*}
n^{-(1+\theta)/2} = 
\frac{2\log(2/\alpha)/(3N)}
{(\theta-1)\sigma_0
\sqrt{2\log(2/\alpha)/N}}=
\frac{\sqrt{2\log(2/\alpha)}}
{3(\theta-1)\sigma_0
\sqrt{N}}
\end{align*}
from which~\eqref{eq:goodn} follows.
\end{proof}

If $n_*$ is not an integer then the
best integer $n$ is either $\lfloor n_*\rfloor$
or $\lceil n_*\rceil$ by the convexity noted in the proof.
If $n_*>N$, then $n=N$ is best. If $n_*<1$, then $n=1$ is best.
That includes the degenerate case with $\sigma_0=0$.
For scrambled Sobol' points it is generally best to
take $n$ to be a power of 2 \cite{Owe22a}, which is not
reflected in the theorem.  We incorporate that restriction
into some numerical examples in Section~\ref{sec:finite}.

For $\theta = 1$, the best $n$ is $n=1$. In that case RQMC is the same as MC, using one uniformly distributed point in $[0,1]$. This result holds for any sampler that gets the ordinary MC rate, even if it has a favorable constant.  In particular, any MC variance reduction techniques that that do not change the convergence rate will not lead to using $n>1$.  They will improve the asymptotic confidence interval width by reducing $\sigma_0$.  \art{[I had this wrong in the oracle rate note.]}

When $\theta>1$, we see that the shortest intervals
come from taking $n = O(N^{1/(\theta+1)})$.
The BVHK rate ($\theta=2$) is then $n=O(N^{1/3})$ while the
rate for smooth integrands ($\theta=3$) is $O(N^{1/4})$.
Note that when the RQMC convergence rate becomes better,
the optimal RQMC sample size to use grows at a slower
rate in $N$.  This may seem like a paradox, but the explanation
can be seen in equation~\eqref{eq:bennettwidth}.
If $\sigma_n$ drops very quickly then the $O(1/\sqrt{R})$ term
it is in does not dominate the other $O(1/R)$ term as much
and a larger $R$ is better.

Because the oracle takes $n$ to be a smaller
power of $N$ for larger $\theta$, we might
wonder why it always picks $n=1$ for the smallest value, $\theta=1$. 
This stems from the factor $\theta-1$ in the constant of proportionality
in \eqref{eq:goodn}.
For fixed $\sigma_0$, $N$ and $\alpha$ we could 
maximize~\eqref{eq:goodn} over $\theta$
to see which rate gives the largest $n$.

What we see from Theorem~\ref{thm:goodn} is that the optimal
$n$ grows slowly with $N$. The variance of the average of $R$
RQMC samples using $n=N/R$ is
$$
O(R^{-1}n^{-\theta}) = O( N^{-1}n^{1-\theta})
= O( N^{-1}N^{(1-\theta)/(1+\theta)})
%= O( N^{-1+(1-\theta)/(1+\theta)})
= O( N^{-2\theta/(1+\theta)}).
$$
By requiring a guaranteed confidence level, we
get a higher variance than $N^{-\theta}$
that we could have had with $R=O(1)$.
For $\theta=2$ the variance rises
from $O(N^{-2})$ to $O(N^{-4/3})$
and for $\theta=3$, it rises from
$O(N^{-3})$ to $O(N^{-3/2})$.
Put another way, obtaining the guaranteed confidence
level raise the variance by $O(N^{2/3})$
and $O(N^{3/2})$ in those two cases.

\art{While we cannot always know the best value of $n$ to use
in a given setting, Theorem~\ref{thm:goodn} gives us some guidance.
The asymptotic value of $\theta$ may be known from theory.
We may also have experience with similar integrands to make a
judgment about what value of $\theta$ is reasonable in a given
setting, or we can do some preliminary sampling to get a working
value of $\theta$ before constructing a betting confidence interval.
For any choice of $\theta$, that interval will be valid, though possibly 
wider than an interval using the true optimal $n$.
Because $0\le Y_i\le1$, we know that $\sigma_0^2\le 1/4$. Therefore
under the model~\eqref{eq:themodel}, Theorem~\ref{thm:goodn}
provides the guidance to take
\begin{align}\label{eq:guidance}
n\le \Bigl(\frac{9(\theta-1)^2N}{8\log(2/\alpha)}\Bigr)^{1/(\theta+1)}.
\end{align}
For example, with $N=2^{10}$ and $\alpha=0.05$, the bound in~\eqref{eq:guidance}
is always below $7$ whenever $1\le\theta\le3$. The best integer
value is $7$ for some values of $\theta$.
For higher confidence levels (smaller $\alpha$), the optimal
$n$ is smaller still.
}


\begin{theorem}
Under the conditions of Theorem~\ref{thm:goodn}
with $\theta>1$ and $\sigma_0>0$, 
let $W_{\mc}$ be the half width using $n=1$
and $W_{\rqmc}$ be the half width using
$n=n_*$ from~\eqref{eq:goodn}. 
Let $N_*=N/\log(2/\alpha)$.
Then for $\theta>1$
\begin{align*}
\frac{W_\rqmc}{W_\mc} 
&= 
\frac{\theta+1}
{\sqrt{{2\sigma^2_0N_*}}+1/3}
\biggl(\frac12
[{3(\theta-1)}]^{1-\theta}
N_*\sigma_0^2\biggr)^{1/(\theta+1)}\\
&=O( N^{\frac{1-\theta}{2\theta+2}})
\end{align*}
as $N\to\infty$.
\end{theorem}
\begin{proof}
The ratio of these widths is
\begin{align*}
\rho=
\frac{W_\rqmc}{W_\mc}
&=
\frac{n^{(1-\theta)/2}\sqrt{2\sigma^2_0\log(2/\alpha)/N}+n\log(2/\alpha)/(3N)}
{\sqrt{{2\sigma^2_0\log(2/\alpha)}/N}+\log(2/\alpha)/(3N)}\\
&=
\frac{n^{(1-\theta)/2}\sqrt{2\sigma^2_0N_*}+n/3}
{\sqrt{{2\sigma^2_0N_*}}+1/3}
\end{align*}
where $N_* = N/\log(2/\alpha)$.
From equation~\eqref{eq:goodn}
\begin{align*}
n^{(1-\theta)/2}
&=
\left(\frac{3(\theta-1)\sigma_0
\sqrt{N}}{\sqrt{2\log(2/\alpha)}}\right)^{(1-\theta)/(\theta+1)}\\
&=({3(\theta-1)\sigma_0\sqrt{N_*/2}})^{(1-\theta)/(\theta+1)}.
\end{align*}
Also, $n=(3(\theta-1)\sigma_0(N_*/2)^{1/2})^{2/(\theta+1)}$.

So for $\theta>1$, the width ratio is
\begin{align*}
\rho&=
\frac{({3(\theta-1)\sigma_0\sqrt{N_*/2}})^{(1-\theta)/(\theta+1)}\sqrt{2\sigma^2_0N_*}
+(3(\theta-1)\sigma_0(N_*/2)^{1/2})^{2/(\theta+1)}/3}
{\sqrt{{2\sigma^2_0N_*}}+1/3}\\
&=\frac{({3(\theta-1)})^{(1-\theta)/(\theta+1)}2^{\theta/(\theta+1)}
+(3(\theta-1))^{2/(\theta+1)}2^{-1/(\theta+1)}/3}
{\sqrt{{2\sigma^2_0N_*}}+1/3}(N_*\sigma_0^2)^{1/(\theta+1)}\\
%&=[{3(\theta-1)}]^{(1-\theta)/(\theta+1)}2^{-1/(\theta+1)}
%\frac{\theta+1}{\sqrt{{2\sigma^2_0N_*}}+1/3}
%(N_*\sigma_0^2)^{1/(\theta+1)}\\
&=\frac{\theta+1}
{\sqrt{{2\sigma^2_0N_*}}+1/3}
\biggl(\frac12
[{3(\theta-1)}]^{1-\theta}
N_*\sigma_0^2\biggr)^{1/(\theta+1)}
\end{align*}
as required.
\end{proof}
\art{There's an easier way to get the rate
of narrowing but it
wouldn't show the constant. We have $n=O(N^{1/(\theta+1)})$
from Theorem~\ref{thm:goodn}. So $R=N/n$ and the width
is $O(R^{-1})=O(N^{1/(\theta+1)-1})=O(N^{\theta/(\theta+1)})$.
Then the narrowing is $O(N^{-\theta/(\theta+1)+1/2})$.

Does somebody want to look at the limit $\theta\downarrow1$?
I'm getting a limit that isn't one, but does approach one
as $N\to\infty$.
}

\aadit{This is what I get: \begin{align*}
\rho=\frac{({3(\theta-1)})^{(1-\theta)/(\theta+1)}(2)^{\theta/(\theta + 1)}
+(3(\theta-1))^{2/(\theta+1)}(2)^{-1/(\theta + 1)}/3}
{\sqrt{{2\sigma^2_0N_*}}+1/3}N_*^{1/(\theta+1)}\sigma_0^{2/(\theta+1)}
\end{align*}

Now, only considering the $N_*$ terms, we get $\Theta(N^{(1 - \theta)/(2(\theta + 1))})$. This agrees with the given examples in the next paragraph.}
\art{Thanks.  I kept putting a wrong sign on one of the $N_*$ exponents and never got around to the 2s.}


For the BVHK case with $\theta=2$, 
we get a ratio of $O(N^{-1/6})$.
For the smooth case with $\theta=3$,
we get a more favorable width ratio of $O(N^{-1/4})$.
The MC widths are $O(N^{-1/2})$, and so for the
BVHK case the RQMC widths are $O(N^{-2/3})$
while for the smooth case, they are $O(N^{-3/4})$.


\section{Finite sample evaluations}\label{sec:finite}

Here we present some finite sample evaluations
of empirical Bernstein based confidence intervals
using RQMC.  We begin with finite $N$ and interval
widths based on Bennett's inequality.  They are
then exact for Bennett's inequality which the
predictable plug-in empirical Bayes confidence intervals
of \cite{WauRam24a} match asymptotically.
After that we present finite sample results on the
actual predictable plug-in empirical Bayes confidence
intervals using code from \art{xxx}.

While our presentation has been for half-widths
we do our numerical examples for the widths
themselves.  The only difference is that the
empirically generated widths reflect the intersection
of confidence intervals with $[0,1]$.  When $Y$ has
an asymmetric distribution, this intersection can
make the interval width different from twice the
empirical Bernstein half-width.
\art{[Aadit: this means that you don't have to change any
of your results that were generated for full widths.]}

\subsection{Semi-empirical evaluations}

We can compute the width of intervals based on Bennett's inequality
for some specific cases.  We saw in Section~\ref{sec:asymptotic}
that the RQMC sample sizes $n$ grow at slow rates
$O(N^{1/3})$ or $O(N^{1/4})$ as the budget $N$ increases.
Here we get a more detailed view of that phenomenon using
the asymptotic formula for some specific variance rates.
We can compute these confidence interval widths for very
large values of $N$ that would be impractical to simulate.

The digital nets of Sobol' \cite{Sob67} under the nested
uniform scramble of \cite{Owe95} have a stratification
property.  The $n$ values of $x_{ij}$ for $i=1,\dots,n$
and any $j$ have one value in each interval $[k/n,(k+1)/n)$
for $k=1,\dots,n$ and those $n$ values are uniformly distributed
and independent of each other. This allows us to work out
the value of $\sigma_0$ and $\theta$ for some illustrative
examples.  The random linear scramble of \cite{Mat98} combined
with a digital shift attains the same variance as the nested
uniform scramble, but it has some dependencies among the stratified
sample values.  In both cases, the $d$ vectors $(x_{1j},
\dots,x_{nj})\in\real^n$ are mutually independent.

We only consider values of $n$ that are powers of $2$
which is a best practice when using Sobol' sequences \cite{Owe22a}.
The smooth error rate $\tilde O(n^{-3/2})$ for the RMSE
is attainable along powers of $2$
but not for general $n$. An elegant argument due
to Sobol' \cite{sobo:1998} explains why.
He notes that
$$
|f(\bsx_{n+1})-\mu|
%= |(n+1)(\hat\mu_{n+1}-\mu)-n(\hat\mu_n-\mu)|
\le (n+1)|\hat\mu_{n+1}-\mu|+n|\hat\mu_n-\mu|.
$$
A rate of $o(n^{-1})$ would imply that $f(\bsx_{n+1})$ itself
consistently estimates $\mu$. For any sampling
algorithm, that could only be true for a very minimal 
set of functions~$f$.
We also take $N$ to be a power of 2.

When $d=1$ we write the variable of integration as $x$.
For $f(x)=x$ we can use the stratified sampling property
to see that $\var(\hat\mu) = 1/(12n^3)$.
In this case $\tilde O(n^{-3})$ is actually $O(n^{-3})$.
This is an example function that attains the smooth
rate as would any one dimensional function for
which $f'(x)^2$ is Riemann integrable.

For $d=1$, the BVHK case arises for any integrand
of bounded variation in the usual sense.
We get $\var(\hat\mu)\le V_{\hk}(f)^2/n^2$
because $D_n^*$ for the stratified sample
cannot be larger than $1/n$. \art{[Let's double check this.]}
For illustration we choose a function with $V_{\hk}(f)=1$
for which we know exactly what the RQMC variance is
for $n$ a power of $2$.
That integrand is $f(x)=1\{x<1/3\}$.
It is constant within $n-1$ of the intervals $[k/n,(k+1)/n)$.
All of this function's variation happens at one jump
however the widths we compute only use the variance
so they apply to any function with the same sampling variances.
By inspecting the base $2$ expansion $1/3=0.010101\cdots$
we can see that the remaining value of $f(x_k)$ equals
$1$ with a probability that is either $1/3$ if $\log_2(n)$
is an odd integer or is $2/3$ if $\log_2(n)$ is an
even integer. It follows that
$\var(\hat\mu_n)=2/(9n^2)$.


\art{The result in Table~\ref{tab:bvhkcase} for $N=1024$
is to choose $n=8$ which violates the guidance that
the best $n$ is at most $7$ when $N=1024$ and $\alpha=0.05$. 
The reason is that this
table restricts to powers of $2$, and $n=8$ is better
than $n=4$.
}

Table~\ref{tab:bvhkcase} shows how the width
minimizing $n$ increase with $N$ in the BVHK
case. These are full widths, not half-widths.
The increase is quite slow. By $N=1024$
the best RQMC sample size is only $n=8$
and this remains the optimal sample size up to $N=4096$.
The value $N^{2/3}W$ very quickly becomes
constant.
Table~\ref{tab:smoothcase} shows the smooth
case.  An RQMC sample size of $n=8$ becomes
optimal for $N=4096$ and remains optimal up
to $N=32768$.  The values $N^{3/4}W$
there are identical. Consistent with the results of
Section~\ref{sec:asymptotic}, the values $n$
grow at a slower rate with $N$ than in the
BVHK case while the widths $W$ decrease at
a faster rate.



\begin{table}[t]\centering
\begin{tabular}{crrcc}
\toprule
   $\log_2(N)$&        $N$ &  $n$ & $W$ & $N^{2/3}W$\\
   \midrule
   \phz0&         1&   1& 5.02e$+$00 &5.020114\\
   \phz4&        16&   2& 7.60e$-$01 &4.826379\\
   \phz7&       128&   4& 1.90e$-$01 &4.826379\\
  10&      1024&   8& 4.75e$-$02 &4.826379\\
  13&      8192&  16& 1.19e$-$02 &4.826379\\
  16&     65536&  32& 2.97e$-$03 &4.826379\\
  19&    524288&  64& 7.42e$-$04 &4.826379\\
  22&   4194304& 128& 1.86e$-$04 &4.826379\\
  25&  33554432& 256& 4.64e$-$05 &4.826379\\
  28& 268435456& 512& 1.16e$-$05 &4.826379\\
 \bottomrule
\end{tabular}
\caption{\label{tab:bvhkcase}
As $N$ varies for $f(x)=1\{x<1/3\}$, this table shows the
value of $n$ that minimizes the width of Bennett's
interval, along with the minimizing width $W$
and $N^{1/6}W$. Each row corresponds to a value of $N$
for which $n$ has increased.}
\end{table}

\begin{table}\centering
\begin{tabular}{crrcc}
\toprule
      $\log_2(N)$&        $N$ &  $n$ & $W$ & $N^{3/4}W$\\
\midrule
  \phz0&         1 &  1 &4.03e$+$00 &4.027454\\
  \phz4&        16 &  2 &5.03e$-$01 &4.027454\\
  \phz8&       256 &  4 &6.29e$-$02 &4.027454\\
 12&      4096 &  8 &7.87e$-$03 &4.027454\\
 16&     65536 & 16 &9.83e$-$04 &4.027454\\
 20&   1048576 & 32 &1.23e$-$04 &4.027454\\
 24&  16777216 & 64 &1.54e$-$05 &4.027454\\
 28& 268435456 &128 &1.92e$-$06 &4.027454\\
 \bottomrule
\end{tabular}
\caption{\label{tab:smoothcase}
As $N$ varies for $f(x)=1$, this table shows the
value of $n$ that minimizes the width of Bennett's
interval, along with the minimizing width $W$
and $N^{3/4}W$. Each row corresponds to a value of $N$
for which $n$ has increased.}
\end{table}

We also looked at the function $f(x)=x\exp(x-1)$
which we revisit in Section~\ref{sec:testfns}.
This function also has $\theta=3$. RQMC with $n$
a power of 2 for this function yields
$$
\var(\hat\mu)\asymp \frac{\int_0^1 f'(x)^2\rd x}{12n^3}
= \frac{5-\exp(-2)}{48^3}.
$$
The values of $N$ at which a new power of $2$
becomes best for this asymptotic rate are exactly
the same as those in Table~\ref{tab:smoothcase}.
That is they are the cases where $\log_2(N)$ is
a multiple of 4, consistent with the $n=N^{1/4}$
rate.  %We have investigated other sequences
%of variances at the rate $n^{-3}$ and while some
%of them start off using larger $n$ than this for
%a given $N$, they have all eventually scaled with
%one in


\subsection{Test functions}\label{sec:testfns}

\art{[Aadit: I'll write here my best guess of what you did
or, based on what we now know, what would have been the 
best thing do do.  We can iterate.]}

\aadit{Are we going to include the simulations regarding the Hedged Betting CI from the paper? All of the simulations are on that and PrPl-EB?}
\art{Yes, I think we should.}

While the betting intervals are asymptotically of the
same width as we would get from Bennett's formula, here
we investigate what choices of $n$ and $R$ give the
narrowest interval widths for some finite $N$. 
We begin with $x\in[0,1]$ and $f(x)=x\exp(x)/\exp(1)$.
This function has minimum $0$ and maximum $1$,  and it is an example
with $\theta=3$. It is not symmetric about $x=1/2$
nor are values $x$ of the form $k/n$ special for it.
Those are factors that beyond variance, could make it
artifically easy for RQMC with Sobol' points.

For $N=2^{10}$ and $n$ equal to powers of two ranging
from $n=1$ to $n=2^9$, we run the plug-in empirical
Bernstein algorithm and compute a confidence interval
based on $R=N/n$ independent RQMC point sets of size $n$ each.
We repeat this process \art{xxx} times independently
and for each value of $n$ record the full confidence
interval width $W$.  The average of these widths is plotted
versus $n$ in Figure xxx.  That figure also shows the
10'th and 90'th percentiles of the width.  The best value
of $n$ for $N=2^{10}$ is xxx. The best value from
Table~\ref{tab:smoothcase} for $f(x)=x$ is $n=4$
and that is also the best value we get for
$\sigma^2_n$ matching the asymptotic rate for this integrand.

The next test function is $f(x)=1\{x<1/3\}$.
This function has the BVHK rate $\theta=1$.
It has a discontinuity at a value that is badly
approximable by integers divided by a power of $2$.
% weird that overleaf does not like: approximable
By contrast $f(x) = 1\{x < 3/8\}$ will be integrated
with zero error by scrambled Sobol' points when $n$
is a multiple of $8$ and it is thus artificially easy.
Figure xxx shows the same quantities for this integrand
that we plotted in Figure xxx for $f(x)=x\exp(x-1)$.

The third test function is
$$
f(\bsx) = 1\{x_1+x_2\ge 2/3\},\quad \bsx\in[0,1]^2.
$$
This function has infinite variation in the sense
of Hardy and Krause.
% based on convergence rates for area of triangles
% it should have $\theta=3/2$. This might also
% be proved by Zhijian He in some article.

\art{At this point I think it would be interesting
to get numbers for a larger value of $N$.  We predict
theoretically that going to $2^{14}$ should double
the best $n$ for the smooth function and that this
should happen at $2^{13}$ for the less smooth function.
Maybe $2^6$ and $2^7$ would also be interesting.}

\art{The figures should also get reference curves versus
$n$ showing a theoretically expected width based on Bennett's inequality.}

\aadit{For $N$, should we use $2^7,2^{10},2^{14}?$}
\art{It depends on how hard it is to run those.  The semi-empirical
work uses a big range of $N$ and then all feasible $n$ for each
of those.  If your computations go fast enough, then you could
do the same up to some $N$, perhaps $2^{14}$ but surely
less than $2^{30}$.  That would let us see at which $N$
the value of $n$ doubles.  It would be interesting to see
when the betting methods start to match what we get from
Bennett's inequality.  Also, if they're slow to match in
optimal $n$, they might still be matching well in width.

Part of the task is to see what interesting things are in
the numerical results beyond the ones we plan for. Often
those surprises lead to new theory. Examples of empirical-first
discoveries: QMC working well for $d>100$ (Paskov and Traub), 
RQMC being better than QMC (me), 
QMC working despite $f$ being unbounded (Sobol's collaborators),
RQMC having such heavy tails that the median over replicates can be
far better than the mean (Zexin Pan and I). There are probably
more that are not at my finger tips.}

\subsection{Ridge functions}

Next we look at the values of $n$ and interval widths
for more complicated RQMC problems with $d>1$. For these,
we do not know the values of $\sigma^2_n$.
We choose to use ridge functions because that makes it
straightforward to very $d$.
For $\bsx\sim\dunif[0,1]^d$ the vector
$\bsz=\Phi^{-1}(\bsx)$ componentwise,
where $\Phi$ is the $\dnorm(0,1)$ cumulative distribution
function has the $\dnorm(0,I)$ distribution over $\real^d$.
Then $\bsone^\tran\bsz/\sqrt{d}\sim\dnorm(0,1)$.
Our ridge function
$$
f(\bsx) =g\Bigl( \frac{\bsone^\tran\Phi^{-1}(\bsx)}{\sqrt{d}}\Bigr).
$$
then has the same mean and variance and differentiability properties
in any dimension $d$.  We choose
\begin{align*}
g_\jmp(v) &= 1\{v\ge1\},\\
g_\knk(v) & = \frac{\min(\max(-2,v),1)+2}3,\\
g_\smo(v) & = \Phi(v),\quad\text{and}\\
g_\fin(v) & = \min\Bigl(1,\frac{\sqrt{\max(v+2,0)}}2\Bigr).
\end{align*}
All of these give ridge functions bounded between $0$ and $1$.
We use dimensions $d\in\{1,2,4,16,64\}$.
They span a very wide range of QMC difficulty.
Using $g_\jmp$ makes $f$ have infinite variation in
the sense of Hardy and Krause for $d\ge2$. 
Using $g_\knk$ makes $f$ have infinite variation in
the sense of Hardy and Krause for $d\ge3$ and yet
it has been seen to be a more `QMC friendly' integrand
than $g_\jmp$ in high dimensions because it has an
asymptotically bounded mean dimension \cite{mdridge},
meaning that it is dominated by low dimensional 
interactions.  Using $g_\smo$ makes $f$ infinitely
differentiable which is generally a very QMC friendly
property.  Using $g_\fin$ gives $f$ a property
similar to some integrands in financial option valuation, namely an
unbounded derivative where $v=-2$.

\section{Discussion}\label{sec:discussion}



In favorable cases
the RQMC variance is $\tilde O(n^{-3})$ and then
using a fixed number $R$ of replicates of
point sets with $n\to\infty$ we can estimate
the integral $\mu$ with a standard deviation 
of $\tilde O(N^{-3/2})$ for $N=nR$ along with
an unbiased estimate of the variance of that estimate.
It remains very difficult to get a confidence 
interval of width $\tilde O(N^{-3/2})$ \cite{err4qmc}.
Using Student's $t$ with a small $R$ gave good
results in an extensive simulation by \cite{LEcEtal24a}
but that success is not yet theoretically understood.
For an integrand subject to known bounds, we can get
a non-asymptotic confidence interval by using $R$
replicates of $n$ RQMC points using a
predictable plug-in empirical Berstein confidence
interval.  In that approach the optimal $n$
grows proportionally to $N^{1/(1+\theta)}$ and
the resulting confidence intervals have width
$O(N^{-\theta/(1+\theta)})$.
It remains to find a reliable way to choose $n$
empirically. 

It also remains to find a way to
use RQMC in confidence sequences.  In numerical
integration problems, if $n$ is not small enough
to get a good answer then $n+1$ will also be
too small to get a good answer. It is more reasonable
to consider a range of values $n$ that grow geometrically
not arithmetically \cite{sobo:1998}. \art{I think this is the one where Sobol said that.}  A confidence sequence for geometrically
growing $n$ would not have to be as wide as one for arithmetically
growing $n$.  \art{I think Sobol' might have looked at this
for the case $\{n,2n\}$.}



\section*{Acknowledgments}

We thank Aaditya Ramdas and Ian Waudby-Smith for helpful
discussions and for the \art{xxx} software
from \art{xxx} that we used.
ABO was supported by the U.S.\ National Science Foundation
grant DMS-2152780.

\art{TODO: We need tidy up references: capitalizations,
consistent name vs initials.}
\bibliographystyle{plain}
\bibliography{FJH25,FJHown25,ebci4rqmc}

\end{document}