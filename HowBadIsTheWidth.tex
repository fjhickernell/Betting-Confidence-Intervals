\documentclass{amsart}
\usepackage{mathtools,upref,siunitx,upquote,fancyvrb,xspace,xcolor}  % changed color to xcolor
\usepackage[hyphens]{url}
\usepackage[utf8]{inputenc}
\usepackage{esdiff}

\input{FJHDef.tex}
\usepackage[authoryear]{natbib} % Enables author-year citations


\usepackage{algpseudocode}
\usepackage{algorithm, algorithmicx}
\algnewcommand\algorithmicparam{\textbf{Parameters:}}
\algnewcommand\PARAM{\item[\algorithmicparam]}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\RETURN{\State \textbf{Return }}

\newcommand{\rd}{\,\mathrm{d}}

\newcommand{\art}[1]{{\color{blue}#1}}

\newcommand{\tkappa}{\widetilde{\kappa}}
\newcommand{\tkmax}{\tkappa_{\max}}

\begin{document}
\title{Constructing a Worst Case for Confidence Intervals of Bounded Random Variables}

\author{Aadit Jain}

\author{Fred J. Hickernell }
\address{Department of Applied Mathematics, Illinois Institute of Technology, Chicago, IL}
\email{hickernell@iit.edu}

\author{Art B. Owen}

\author{Aleksei Sorokin}

\begin{abstract} TBD
\end{abstract}

\maketitle

Define a random variable $Y$ such that 
\begin{equation*}
    \Prob(Y = 1) = p, \quad \Prob(Y=0) = 1-p, \qquad \text{for some } p \in [0,1].
\end{equation*}
Note that 
\begin{align*}
    \mu : = \Ex(Y) & = p, \\
    \Ex(Y^2) & = p, \\
    \sigma^2 : = \var(Y) & = \Ex(Y^2) - \mu^2 = p - p^2 = p(1-p)\\
    \Ex(Y^3) & = \Ex(Y) =  p, \\
    \Ex[(Y - \mu)^4] & = p(1-p)^4 + (1-p)(0 - p)^4 \\
    & = p(1 - p)[(1-p)^3 + p^3] \\
    & = p(1-p)(1 - 3p + 3 p^2) \\
    \kappa : = \frac{\Ex[(Y - \mu)^4]}{\sigma^4} & = 
    \frac{1 - 3p + 3 p^2}{p(1-p)}
\end{align*}
Thus, the kurtosis can range from $1$, when $p = 1/2$ to $\infty$ when $p = 0$ or $p=1$.


Suppose we take an IID sample $Y_1, \ldots, Y_m$.  Let $L$ of the values be $1$ and $m - L$ be $0$.  Then our sample mean is 
\begin{equation*}
    \hmu_m := \frac 1m \sum_{i=1}^m Y_i 
    = \frac{L}{m} =  \hp
\end{equation*}
where $\hp : = L/m$.

Can we show that for some choice of $p$
\[
\Prob\Bigl(\lvert \mu - \hmu_m \rvert > \frac{c \log(1/\alpha)}{m} \Bigr) \ge \alpha
\]
for some $c$, meaning that the $1-\alpha$ confidence interval will always be $\Order(\log(1/\alpha)/n)$ for bounded random variables?

\art{There are lots of inequalities for a binomial random variable if we take $Y_i\in\{0,1\}$ instead. Doing that won't change the kurtosis. Shorack and Wellner's empirical processes book has some inequalities on page 440. However, few of those are about lower bounds on tail probabilities. They mostly do upper bounds. They like to handle one tail at a time.  Maybe we can find a bigger collection of inequalities for the binomial.  I'm a bit puzzled by the $m$ in the denominator where I was expecting $\sqrt{m}$. BTW, if you're ever looking for inequalities about the Poisson distribution, that book has some very nice ones. }

This is equivalent to
\[
\Prob\Bigl(\lvert \mu - \hmu_m \rvert > \frac{c \beta}{m} \Bigr) \ge \exp(-\beta)
\]

Note that 
\[
m(\mu - \hmu_m)  = m(p - \hp) = \ell - L
\]
where $\ell : = mp$ is not necessarily an integer.
Moreover, 
\begin{align*}
    \MoveEqLeft \Prob(\lvert \mu - \hmu_m \rvert > b/m) \\
    &= \Prob[ \ell - L < -b/2 \text{ or } \ell - L >  b/2] \\
    &= \Prob[ L < \ell - b/2 \text{ or } L > \ell + b/2] \\
    & = \sum_{0 \le l  < \ell - b/2}  \binom{m}{l} p^{l}(1-p)^{m-l} 
    + \sum_{\ell + b/2 < l \le m}  \binom{m}{l} p^l(1-p)^{n-l}
\end{align*}

\bigskip

BELOW IS A WORK IN PROGRESS


Thus, e.g., for $p < 1/N$ and $b \le 2pN $
\[
\Prob(\lvert \mu - \hmu_N \rvert > 2/N) 
> (1-p)^{N} 
\]

Let $f(x) = \exp(Nx) - (1-x)^N$.  Note that $f(0) =0$ and 
\[
f'(x) = N \exp(Nx) - N(1-x)^{N-1}
\]

\section{How Bad Are the CIs for Bounded Kurtosis}

\cite{HicEtal14a} derived a finite sample confidence interval assuming a bound on the kurtosis.  We are going to take advantage of that work.

In \cite{HicEtal14a} it was assumed that one would set the confidence interval the inflation factor for the variance, and the number of samples to bound the variance.  Then the maximum kurtosis was implied.  We will change so that the maximum kurtosis is set, and this determines the number of samples required to bound the variance.

Let $1-\alpha$ be our confidence level for the whole algorithm, and let $(1 - \talpha)^2 \le 1 - \alpha$, i.e., $\talpha = 1 - \sqrt{1-\alpha}$ or even $\talpha = \alpha/2$.   \citet[(12)]{HicEtal14a} implies that 
\begin{subequations} \label{sampvarbd}
	\begin{gather}\label{sampvarup}
		\Prob\left[s^2_n < \sigma^2 \left\{1 + \sqrt{\frac{\tilde\kappa}{\talpha n}}\right\} \right] \ge 1 - \talpha, \\
		\label{sampvarlo}
		\Prob\left[s^2_n > \sigma^2 \left\{1 - \sqrt{\frac{\tilde\kappa}{\talpha n}}\right\} \right] \ge 1 - \talpha.
	\end{gather}
\end{subequations}
(I have simplified the expression, while still maintaining its validity.)  If $\fC^2$ is our chosen inflation factor for the variance, then 
\begin{equation}
\label{sampvarlo_b}
\Prob[\fC^2 s^2_n > \sigma^2] \ge 1 - \talpha \quad \text{for}  \quad \fC^2 \ge \frac{1}{1 - \sqrt{\frac{\tilde\kappa }{\talpha n}}} 
\end{equation}
Suppose that we fix $\fC$ for some value greater than one, and we assume that $\tkappa$ is no greater than some bound, $\tkmax$.  Then we have $\Prob[\fC^2 s^2_{n_\sigma}> \sigma^2] \ge 1 - \talpha $ for  
\begin{equation}
	n_\sigma := \left \lceil \frac{\tkmax\fC^4}{\talpha(\fC^2-1)^2} \right \rceil.
\end{equation}
E.g. if $\fC = 2$ and $\talpha = 0.005$, then $n_\sigma = 356 \tkmax$.

For the Chebychev inequality \citet[(14)]{HicEtal14a} tells us that we then need an additional 
\[
N_{\text{Cheb}}(\varepsilon,s_{n_\sigma},\talpha)
= 
\left\lceil\frac{\fC^2s_{n_\sigma}^2}{\talpha\varepsilon^2}\right\rceil
\]
Putting these together gets us a total cost of 
\[
\left \lceil \frac{\tkmax\fC^4}{\talpha(\fC^2-1)^2} \right \rceil + \left\lceil\frac{\fC^2s_{n_\sigma}^2}{\talpha\varepsilon^2}\right\rceil.
\]
You pay an $\Order(\tkmax)$ cost up front for a large kurtosis, but then the remainder of the cost is $\Order(\sigma^2/\varepsilon^2)$ to get the CI width to be $\varepsilon$.

Other work to be done, which should be easy:
\begin{itemize}
	\item The cost is random because $s_{n_\sigma}$ is random.  Determine the upper bound on the cost with high probability.  
	\item Extend this the Berry-Esseen bounds rather than Chebychev.
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{FJH25,FJHown25}

\end{document}