\documentclass{amsart}
\usepackage{mathtools,upref,siunitx,upquote,fancyvrb,xspace,xcolor}  % changed color to xcolor
\usepackage[hyphens]{url}
\usepackage[utf8]{inputenc}
\usepackage{esdiff}

\input{FJHDef.tex}


\usepackage{algpseudocode}
\usepackage{algorithm, algorithmicx}
\algnewcommand\algorithmicparam{\textbf{Parameters:}}
\algnewcommand\PARAM{\item[\algorithmicparam]}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\RETURN{\State \textbf{Return }}

\newcommand{\rd}{\,\mathrm{d}}

\newcommand{\art}[1]{{\color{blue}#1}}


\begin{document}
\title{Constructing a Worst Case for Confidence Intervals of Bounded Random Variables}

\author{Aadit Jain}

\author{Fred J. Hickernell }
\address{Department of Applied Mathematics, Illinois Institute of Technology, Chicago, IL}
\email{hickernell@iit.edu}

\author{Art B. Owen}

\author{Aleksei Sorokin}

\begin{abstract} TBD
\end{abstract}

\maketitle

Define a random variable $Y$ such that 
\begin{equation*}
    \Prob(Y = 1) = p, \quad \Prob(Y=0) = 1-p, \qquad \text{for some } p \in [0,1].
\end{equation*}
Note that 
\begin{align*}
    \mu : = \Ex(Y) & = p, \\
    \Ex(Y^2) & = p, \\
    \sigma^2 : = \var(Y) & = \Ex(Y^2) - \mu^2 = p - p^2 = p(1-p)\\
    \Ex(Y^3) & = \Ex(Y) =  p, \\
    \Ex[(Y - \mu)^4] & = p(1-p)^4 + (1-p)(0 - p)^4 \\
    & = p(1 - p)[(1-p)^3 + p^3] \\
    & = p(1-p)(1 - 3p + 3 p^2) \\
    \kappa : = \frac{\Ex[(Y - \mu)^4]}{\sigma^4} & = 
    \frac{1 - 3p + 3 p^2}{p(1-p)}
\end{align*}
Thus, the kurtosis can range from $1$, when $p = 1/2$ to $\infty$ when $p = 0$ or $p=1$.


Suppose we take an IID sample $Y_1, \ldots, Y_m$.  Let $L$ of the values be $1$ and $m - L$ be $0$.  Then our sample mean is 
\begin{equation*}
    \hmu_m := \frac 1m \sum_{i=1}^m Y_i 
    = \frac{L}{m} =  \hp
\end{equation*}
where $\hp : = L/m$.

Can we show that for some choice of $p$
\[
\Prob\Bigl(\lvert \mu - \hmu_m \rvert > \frac{c \log(1/\alpha)}{m} \Bigr) \ge \alpha
\]
for some $c$, meaning that the $1-\alpha$ confidence interval will always be $\Order(\log(1/\alpha)/n)$ for bounded random variables?

\art{There are lots of inequalities for a binomial random variable if we take $Y_i\in\{0,1\}$ instead. Doing that won't change the kurtosis. Shorack and Wellner's empirical processes book has some inequalities on page 440. However, few of those are about lower bounds on tail probabilities. They mostly do upper bounds. They like to handle one tail at a time.  Maybe we can find a bigger collection of inequalities for the binomial.  I'm a bit puzzled by the $m$ in the denominator where I was expecting $\sqrt{m}$. BTW, if you're ever looking for inequalities about the Poisson distribution, that book has some very nice ones. }

This is equivalent to
\[
\Prob\Bigl(\lvert \mu - \hmu_m \rvert > \frac{c \beta}{m} \Bigr) \ge \exp(-\beta)
\]

Note that 
\[
m(\mu - \hmu_m)  = m(p - \hp) = \ell - L
\]
where $\ell : = mp$ is not necessarily an integer.
Moreover, 
\begin{align*}
    \MoveEqLeft \Prob(\lvert \mu - \hmu_m \rvert > b/m) \\
    &= \Prob[ \ell - L < -b/2 \text{ or } \ell - L >  b/2] \\
    &= \Prob[ L < \ell - b/2 \text{ or } L > \ell + b/2] \\
    & = \sum_{0 \le l  < \ell - b/2}  \binom{m}{l} p^{l}(1-p)^{m-l} 
    + \sum_{\ell + b/2 < l \le m}  \binom{m}{l} p^l(1-p)^{n-l}
\end{align*}

\bigskip

BELOW IS A WORK IN PROGRESS


Thus, e.g., for $p < 1/N$ and $b \le 2pN $
\[
\Prob(\lvert \mu - \hmu_N \rvert > 2/N) 
> (1-p)^{N} 
\]

Let $f(x) = \exp(Nx) - (1-x)^N$.  Note that $f(0) =0$ and 
\[
f'(x) = N \exp(Nx) - N(1-x)^{N-1}
\]

\section{How Bad Are the CIs for Bounded Kurtosis}

\cite{HicEtal14a} derived a finite sample confidence interval assuming a bound on the kurtosis.  We are going to take advantage of that work.




\bibliographystyle{amsalpha}
\bibliography{FJH25,FJHown25}

\end{document}