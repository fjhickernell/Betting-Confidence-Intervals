\documentclass{amsart}
\usepackage{mathtools,upref,siunitx,upquote,fancyvrb,xspace,xcolor}  % changed color to xcolor
\usepackage[hyphens]{url}
\usepackage[utf8]{inputenc}
\usepackage{esdiff}

\input{FJHDef.tex}
\usepackage[authoryear]{natbib} % Enables author-year citations


\usepackage{algpseudocode}
\usepackage{algorithm, algorithmicx}
\algnewcommand\algorithmicparam{\textbf{Parameters:}}
\algnewcommand\PARAM{\item[\algorithmicparam]}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\RETURN{\State \textbf{Return }}

\newcommand{\rd}{\,\mathrm{d}}

\newcommand{\art}[1]{{\color{blue}#1}}

\newcommand{\tkappa}{\widetilde{\kappa}}
\newcommand{\tkmax}{\tkappa_{\max}}

\begin{document}
\title{Constructing a Worst Case for Confidence Intervals of Bounded Random Variables}

\author{Aadit Jain}

\author{Fred J. Hickernell }
\address{Department of Applied Mathematics, Illinois Institute of Technology, Chicago, IL}
\email{hickernell@iit.edu}

\author{Art B. Owen}

\author{Aleksei Sorokin}

\begin{abstract} TBD
\end{abstract}

\maketitle

\section{How Bad Are the CIs for Bounded Kurtosis}

\cite{HicEtal14a} derived a finite sample confidence interval assuming a bound on the kurtosis.  We want to understand the penalty that one might pay for having a large kurtosis.  

In \cite{HicEtal14a} it was assumed that one would set the confidence interval the inflation factor for the variance, and the number of samples to bound the variance.  Then the maximum kurtosis was implied.  We will change so that the maximum kurtosis and the inflation factor are set, and this determines the number of samples required to bound the variance.

Let $Y$ be a random variable with mean $\mu$, variance $\sigma$, and modified kurtosis (normalized fourth moment)
\[
\tkappa : = \frac{\Ex[(Y - \mu)^4]}{\sigma^2}
\]
Let $1-\alpha$ be our confidence level for the whole algorithm, and let $(1 - \talpha)^2 \le 1 - \alpha$, i.e., $\talpha = 1 - \sqrt{1-\alpha}$ or even $\talpha = \alpha/2$.   \citet[(12)]{HicEtal14a} implies that 
\begin{subequations} \label{sampvarbd}
	\begin{gather}\label{sampvarup}
		\Prob\left[s^2_n < \sigma^2 \left\{1 + \sqrt{\frac{\tilde\kappa}{\talpha n}}\right\} \right] \ge 1 - \talpha, \\
		\label{sampvarlo}
		\Prob\left[s^2_n > \sigma^2 \left\{1 - \sqrt{\frac{\tilde\kappa}{\talpha n}}\right\} \right] \ge 1 - \talpha,
	\end{gather}
\end{subequations}
where $s_n^2$ denote the unbiased sample variance.  (I have simplified the expression, while still maintaining its validity.)  
If $\fC^2$ is our chosen inflation factor for the variance, then 
\begin{equation}
\label{sampvarlo_b}
\Prob[\fC^2 s^2_n > \sigma^2] \ge 1 - \talpha \quad \text{for}  \quad \fC^2 \ge \frac{1}{1 - \sqrt{\frac{\tilde\kappa }{\talpha n}}} .
\end{equation}


Suppose that we fix $\fC$ for some value greater than one, and we assume that $\tkappa$ is no greater than some bound, $\tkmax$.  Then we have $\Prob[\fC^2 s^2_{n_\sigma}> \sigma^2] \ge 1 - \talpha $ for  
\begin{equation}
	n_\sigma := \left \lceil \frac{\tkmax\fC^4}{\talpha(\fC^2-1)^2} \right \rceil.
\end{equation}
E.g. if $\fC = 2$ and $\talpha = 0.005$, then $n_\sigma = 356 \tkmax$.

\cite[Theorem 6]{HicEtal14a} gives an upper bound of the total cost of this algorithm with probability $1-\beta$ to obtain a half-width of $\varepsilon$ as 
\begin{multline}
    N_{\textrm{up}}
    : = \left \lceil \frac{\tkmax\fC^4}{\talpha(\fC^2-1)^2} \right \rceil 
    \\
    + \max\bigl(1,N_{\text{Cheb}}(\varepsilon,\sigma v(\talpha,\beta, \fC), \talpha), N_{\text{BE}}(\varepsilon,\sigma v(\talpha,\beta, \fC), \talpha, \tkappa^{3/4}) \bigr)
\end{multline}
where 
\begin{align*}
    v^2(\talpha,\beta,\fC) = \fC^2 + (\fC-1)^2 \sqrt{\frac{\talpha}{\beta}}.
\end{align*}

For the Chebychev inequality \citet[(14)]{HicEtal14a} tells us that we then need an additional 
\[
N_{\text{Cheb}}(\varepsilon,\sigma v(\talpha,\beta, \fC),\talpha)
= 
\left\lceil\frac{\fC^2 \sigma^2 v^2(\talpha,\beta,\fC)}{\talpha\varepsilon^2}\right\rceil.
\]

For the Berry-Esseen inequality, from \cite[(16)]{HicEtal14a} it follows that 
\begin{multline*}
    N_{\text{BE}}(\varepsilon,\sigma v(\talpha,\beta, \fC), \talpha, \tkappa^{3/4}) \\
    = \min\Bigl\{n \in \naturals : \Phi\bigl(-\sqrt{n} \varepsilon/(\sigma v(\talpha,\beta,\fC)\bigr) + \Delta_n \bigl(\sqrt{n} \varepsilon/(\sigma v(\talpha,\beta,\fC)),\tkappa^{3/4}\bigr) \le \talpha/2 \Bigr\}
\end{multline*}
where 
\begin{equation*}
    \Delta_n(x,\tkappa^{3/4}) = \frac{1}{\sqrt{n}} \min \biggl( A_1(\tkappa^{3/4} + A_2), \frac{A_3 \tkappa^{3/4}}{1 + \abs{x}^3} \biggr)
\end{equation*}
[We may be able to make this more beautiful, but a bit coarser.]


You pay an $\Order(\tkmax)$ cost up front for a large kurtosis, but then the remainder of the cost is $\Order(\sigma^2/\varepsilon^2)$ to get the CI width to be $\varepsilon$.


\section{What Can We Learn from the Bernoulli Random Variable?}


Define a random variable $Y$ such that 
\begin{equation*}
    \Prob(Y = 1) = p, \quad \Prob(Y=0) = 1-p, \qquad \text{for some } p \in [0,1].
\end{equation*}
Note that 
\begin{align*}
    \mu : = \Ex(Y) & = p, \\
    \Ex(Y^2) & = p, \\
    \sigma^2 : = \var(Y) & = \Ex(Y^2) - \mu^2 = p - p^2 = p(1-p)\\
    \Ex(Y^3) & = \Ex(Y) =  p, \\
    \Ex[(Y - \mu)^4] & = p(1-p)^4 + (1-p)(0 - p)^4 \\
    & = p(1 - p)[(1-p)^3 + p^3] \\
    & = p(1-p)(1 - 3p + 3 p^2) \\
    \kappa : = \frac{\Ex[(Y - \mu)^4]}{\sigma^4} & = 
    \frac{1 - 3p + 3 p^2}{p(1-p)}
\end{align*}
Thus, the kurtosis can range from $1$, when $p = 1/2$ to $\infty$ when $p = 0$ or $p=1$.


Suppose we take an IID sample $Y_1, \ldots, Y_m$.  Let $L$ of the values be $1$ and $m - L$ be $0$.  Then our sample mean is 
\begin{equation*}
    \hmu_m := \frac 1m \sum_{i=1}^m Y_i 
    = \frac{L}{m} =  \hp
\end{equation*}
where $\hp : = L/m$.

Can we show that for some choice of $p$
\[
\Prob\Bigl(\lvert \mu - \hmu_m \rvert > \frac{c \log(1/\alpha)}{m} \Bigr) \ge \alpha
\]
for some $c$, meaning that the $1-\alpha$ confidence interval will always be $\Order(\log(1/\alpha)/n)$ for bounded random variables?

\art{No confidence interval based on an IID sample can be asymptotically smaller than the one you get from the CLT.  That might be in some of the Ramdas work. Last time I saw it, it was in \cite{aust:mack:2022-tr}. Lester Mackey is this year's COPSS award winner (big prize for statistician aged no more than 40.}


%\art{There are lots of inequalities for a binomial random variable if we take $Y_i\in\{0,1\}$ instead. Doing that won't change the kurtosis. Shorack and Wellner's empirical processes book has some inequalities on page 440. However, few of those are about lower bounds on tail probabilities. They mostly do upper bounds. They like to handle one tail at a time.  Maybe we can find a bigger collection of inequalities for the binomial.  I'm a bit puzzled by the $m$ in the denominator where I was expecting $\sqrt{m}$. BTW, if you're ever looking for inequalities about the Poisson distribution, that book has some very nice ones. }

This is equivalent to
\[
\Prob\Bigl(\lvert \mu - \hmu_m \rvert > \frac{c \beta}{m} \Bigr) \ge \exp(-\beta)
\]

Note that 
\[
m(\mu - \hmu_m)  = m(p - \hp) = \ell - L
\]
where $\ell : = mp$ is not necessarily an integer.
Moreover, 
\begin{align*}
    \MoveEqLeft \Prob(\lvert \mu - \hmu_m \rvert > b/m) \\
    &= \Prob[ \ell - L < -b/2 \text{ or } \ell - L >  b/2] \\
    &= \Prob[ L < \ell - b/2 \text{ or } L > \ell + b/2] \\
    & = \sum_{0 \le l  < \ell - b/2}  \binom{m}{l} p^{l}(1-p)^{m-l} 
    + \sum_{\ell + b/2 < l \le m}  \binom{m}{l} p^l(1-p)^{n-l}
\end{align*}

\bigskip

BELOW IS A WORK IN PROGRESS


Thus, e.g., for $p < 1/N$ and $b \le 2pN $
\[
\Prob(\lvert \mu - \hmu_N \rvert > 2/N) 
> (1-p)^{N} 
\]

Let $f(x) = \exp(Nx) - (1-x)^N$.  Note that $f(0) =0$ and 
\[
f'(x) = N \exp(Nx) - N(1-x)^{N-1}
\]


%\section{Fred worried about independence}

\bibliographystyle{plainnat}
\bibliography{FJH25,FJHown25,ebci4rqmc}

\end{document}